{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vithurshiniS/Twitter-Sentiment-Analysis/blob/main/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAm6N9ypFMJJ",
        "outputId": "1bddb9cb-fa2f-412e-fdbf-1465410bdfd9"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BilQeFzMjH36"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxNxT214cVCb"
      },
      "source": [
        "Language Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uHNjaWGhsOt",
        "outputId": "ca16a424-d94d-4b77-b5ed-08bb9f48a568"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import string\n",
        "import pickle\n",
        "import nltk\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.base import TransformerMixin \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBsLw5-5cauU"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoSofuSWFQOG",
        "outputId": "d35a7d68-63c7-4999-89b8-442262bfb43e"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "#from nltk.corpus import movie_reviews\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "class VoteClassifier(ClassifierI):\n",
        "  def __init__(self, *classifiers):\n",
        "    self._classifiers = classifiers\n",
        "\n",
        "  def classify(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    return mode(votes)\n",
        "\n",
        "  def confidence(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "\n",
        "    choice_votes = votes.count(mode(votes))\n",
        "    conf = choice_votes / len(votes)\n",
        "    return conf\n",
        "    \n",
        "short_pos = open(\"/content/drive/MyDrive/Research/positive_en.txt\",\"r\",encoding=\"latin-1\").read()\n",
        "short_neg = open(\"/content/drive/MyDrive/Research/negative_en.txt\",\"r\",encoding=\"latin-1\").read()\n",
        "\n",
        "# move this up here\n",
        "all_words = []\n",
        "documents = []\n",
        "\n",
        "\n",
        "#  j is adject, r is adverb, and v is verb\n",
        "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
        "allowed_word_types = [\"J\"]\n",
        "\n",
        "for p in short_pos.split('\\n'):\n",
        "  documents.append( (p, \"pos\") )\n",
        "  words = word_tokenize(p)\n",
        "  pos = nltk.pos_tag(words)\n",
        "  for w in pos:\n",
        "    if w[1][0] in allowed_word_types:\n",
        "      all_words.append(w[0].lower())\n",
        "\n",
        "    \n",
        "for p in short_neg.split('\\n'):\n",
        "  documents.append( (p, \"neg\") )\n",
        "  words = word_tokenize(p)\n",
        "  pos = nltk.pos_tag(words)\n",
        "  for w in pos:\n",
        "    if w[1][0] in allowed_word_types:\n",
        "      all_words.append(w[0].lower())\n",
        "\n",
        "\n",
        "\n",
        "save_documents = open(\"/content/drive/MyDrive/Research/documents.pickle\",\"wb\")\n",
        "pickle.dump(documents, save_documents)\n",
        "save_documents.close()\n",
        "\n",
        "\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "\n",
        "word_features = list(all_words.keys())[:5000]\n",
        "\n",
        "\n",
        "save_word_features = open(\"/content/drive/MyDrive/Research/word_features5k.pickle\",\"wb\")\n",
        "pickle.dump(word_features, save_word_features)\n",
        "save_word_features.close()\n",
        "\n",
        "\n",
        "def find_features(document):\n",
        "  words = word_tokenize(document)\n",
        "  features = {}\n",
        "  for w in word_features:\n",
        "    features[w] = (w in words)\n",
        "\n",
        "  return features\n",
        "\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
        "\n",
        "random.shuffle(featuresets)\n",
        "print(len(featuresets))\n",
        "\n",
        "testing_set = featuresets[10000:]\n",
        "training_set = featuresets[:10000]\n",
        "\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
        "classifier.show_most_informative_features(15)\n",
        "\n",
        "###############\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/originalnaivebayes5k.pickle\",\"wb\")\n",
        "pickle.dump(classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
        "MNB_classifier.train(training_set)\n",
        "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/MNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(MNB_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
        "BernoulliNB_classifier.train(training_set)\n",
        "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/BernoulliNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
        "LogisticRegression_classifier.train(training_set)\n",
        "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/LogisticRegression_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "\n",
        "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
        "LinearSVC_classifier.train(training_set)\n",
        "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/LinearSVC_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(LinearSVC_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "\n",
        "##NuSVC_classifier = SklearnClassifier(NuSVC())\n",
        "##NuSVC_classifier.train(training_set)\n",
        "##print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
        "\n",
        "\n",
        "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
        "SGDC_classifier.train(training_set)\n",
        "print(\"SGDClassifier accuracy percent:\",nltk.classify.accuracy(SGDC_classifier, testing_set)*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/SGDC_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(SGDC_classifier, save_classifier)\n",
        "save_classifier.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10664\n",
            "Original Naive Bayes Algo accuracy percent: 71.3855421686747\n",
            "Most Informative Features\n",
            "              engrossing = True              pos : neg    =     20.9 : 1.0\n",
            "                  boring = True              neg : pos    =     19.5 : 1.0\n",
            "                   flaws = True              pos : neg    =     16.2 : 1.0\n",
            "                mediocre = True              neg : pos    =     15.7 : 1.0\n",
            "                 routine = True              neg : pos    =     15.1 : 1.0\n",
            "                    flat = True              neg : pos    =     14.7 : 1.0\n",
            "              refreshing = True              pos : neg    =     13.6 : 1.0\n",
            "               inventive = True              pos : neg    =     13.6 : 1.0\n",
            "                    warm = True              pos : neg    =     12.5 : 1.0\n",
            "               wonderful = True              pos : neg    =     12.5 : 1.0\n",
            "                powerful = True              pos : neg    =     12.4 : 1.0\n",
            "                touching = True              pos : neg    =     11.2 : 1.0\n",
            "                mindless = True              neg : pos    =     11.1 : 1.0\n",
            "                   stale = True              neg : pos    =     11.1 : 1.0\n",
            "             mesmerizing = True              pos : neg    =     10.9 : 1.0\n",
            "MNB_classifier accuracy percent: 71.3855421686747\n",
            "BernoulliNB_classifier accuracy percent: 71.6867469879518\n",
            "LogisticRegression_classifier accuracy percent: 71.23493975903614\n",
            "LinearSVC_classifier accuracy percent: 69.27710843373494\n",
            "SGDClassifier accuracy percent: 70.93373493975903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGwD9z7-csnC"
      },
      "source": [
        "Creating sentiment module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GqX6M5tMIHf",
        "outputId": "a22a8a8d-e53d-45db-f7ad-37141ccccfdb"
      },
      "source": [
        "#File: sentiment_mod.py\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "#from nltk.corpus import movie_reviews\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "class VoteClassifier(ClassifierI):\n",
        "  def __init__(self, *classifiers):\n",
        "    self._classifiers = classifiers\n",
        "\n",
        "  def classify(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    return mode(votes)\n",
        "\n",
        "  def confidence(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "\n",
        "    choice_votes = votes.count(mode(votes))\n",
        "    conf = choice_votes / len(votes)\n",
        "    return conf\n",
        "\n",
        "\n",
        "documents_f = open(\"/content/drive/MyDrive/Research/documents.pickle\", \"rb\")\n",
        "documents = pickle.load(documents_f)\n",
        "documents_f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_features5k_f = open(\"/content/drive/MyDrive/Research/word_features5k.pickle\", \"rb\")\n",
        "word_features = pickle.load(word_features5k_f)\n",
        "word_features5k_f.close()\n",
        "\n",
        "\n",
        "def find_features(document):\n",
        "  words = word_tokenize(document)\n",
        "  features = {}\n",
        "  for w in word_features:\n",
        "    features[w] = (w in words)\n",
        "\n",
        "  return features\n",
        "\n",
        "\n",
        "\n",
        "# featuresets_f = open(\"/content/drive/MyDrive/Research_2021/featuresets.pickle\", \"rb\")\n",
        "# featuresets = pickle.load(featuresets_f)\n",
        "# featuresets_f.close()\n",
        "\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
        "\n",
        "random.shuffle(featuresets)\n",
        "print(len(featuresets))\n",
        "\n",
        "testing_set = featuresets[10000:]\n",
        "training_set = featuresets[:10000]\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/originalnaivebayes5k.pickle\", \"rb\")\n",
        "classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/MNB_classifier5k.pickle\", \"rb\")\n",
        "MNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/BernoulliNB_classifier5k.pickle\", \"rb\")\n",
        "BernoulliNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/LogisticRegression_classifier5k.pickle\", \"rb\")\n",
        "LogisticRegression_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/LinearSVC_classifier5k.pickle\", \"rb\")\n",
        "LinearSVC_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/SGDC_classifier5k.pickle\", \"rb\")\n",
        "SGDC_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "voted_classifier = VoteClassifier(classifier,LinearSVC_classifier,MNB_classifier,BernoulliNB_classifier,LogisticRegression_classifier)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sentiment(text):\n",
        "  feats = find_features(text)\n",
        "  return voted_classifier.classify(feats),voted_classifier.confidence(feats)\n",
        "\n",
        "\n",
        "\n",
        "print(sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
        "print(sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10664\n",
            "('pos', 1.0)\n",
            "('neg', 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBhv-4lc7H7"
      },
      "source": [
        "Importing sentiment module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr13im9AEuEb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LvW28JOEt1F"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpSEqRGQSfy",
        "outputId": "282946f8-f036-4c76-970f-9ef359972a6c"
      },
      "source": [
        "!ls /content/drive/MyDrive/Research/sent_mod1.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/sent_mod1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPS0KrBmRBWW"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/Research')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga9bhhlLQmdK",
        "outputId": "255be099-d064-41b0-b892-9a036980d3f0"
      },
      "source": [
        "import sent_mod1 as s\n",
        "\n",
        "print(s.sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
        "print(s.sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10664\n",
            "('pos', 1.0)\n",
            "('neg', 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8ZdhBGAdFtJ"
      },
      "source": [
        "Sentiment Analysis without language detection\n",
        "Social Media Streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qWupp5aBSkeh",
        "outputId": "fafd101f-cb9f-438d-cc3e-350bd1cc93bf"
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import sent_mod1 as s\n",
        "\n",
        "#consumer key, consumer secret, access token, access secret.\n",
        "ckey=\"xxxxxxxxxxxxxxxxxxxxx\"\n",
        "csecret=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "atoken=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "asecret=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "#from twitterapistuff import *\n",
        "\n",
        "class listener(StreamListener):\n",
        "  def on_data(self, data):\n",
        "    all_data = json.loads(data)\n",
        "    tweet = all_data[\"text\"]\n",
        "    sentiment_value, confidence = s.sentiment(tweet)\n",
        "    print(tweet, sentiment_value)\n",
        "\n",
        "\t  # if confidence*100 >= 80:\n",
        "    #   output = open(\"twitter-out.txt\",\"a\")\n",
        "    #   output.write(sentiment_value)\n",
        "    #   output.write('\\n')\n",
        "    #   output.close()\n",
        "\n",
        "      #return True\n",
        "\n",
        "  def on_error(self, status):\n",
        "    print(status)\n",
        "\n",
        "auth = OAuthHandler(ckey, csecret)\n",
        "auth.set_access_token(atoken, asecret)\n",
        "\n",
        "twitterStream = Stream(auth, listener())\n",
        "twitterStream.filter(track=[\"valimai\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongside…\n",
            "RT @WasimJaffer14: Captain, Opener, No.3, Wicket Keeper, ICC event winning coach, former NCA head, and now India coach. But above all a gre…\n",
            "RT @FCM_Production: 01 Year of Master 🥺😎\n",
            "@actorvijay #Master \n",
            ".\n",
            "#Beast #BeastFirstSingle \n",
            "#BeastUpdate https://t.co/0Eb9HsCgTI\n",
            "RT @TeaPainUSA: All over America tonight, people that think they’re part of a master race are drinkin’ their own pee.\n",
            "RT @42M4rvin: Read the rest of the article! https://t.co/YvjA5C8kqd\n",
            "fisting #balls #ballbusting #ballbashing #punch #big\n",
            "#balls #bust #fist #hung #dick #jock #ballslick #ballbuster… https://t.co/y7n9NMCNfn\n",
            "RT @TeaPainUSA: All over America tonight, people that think they’re part of a master race are drinkin’ their own pee.\n",
            "RT @LuvDatta_INC: In Political Science we studied that a bunch of Ministers Sail together &amp; Sink together. Nowadays Godi Media too is inclu…\n",
            "@setsuna_maimai Fragrance Re:MASTER ミラー↺\n",
            "承諾してくれてありがとうございます…！！\n",
            "RT @wizardingworld: Happy Birthday to Hogwarts Potions master, Severus Snape 🐍 https://t.co/gCefVS6pyo\n",
            "I love this letter... and this website @facinghistory Letter from Jourdon Anderson: A Freedman Writes His Former Ma… https://t.co/IHxY4FsmKu\n",
            "@manuelaxntj Selo gada master\n",
            "RT @lovelySHEIK3: Hiii friends I AM BACK 😅😎\n",
            "#master #Beast @actorvijay https://t.co/YwdG8sGmnK\n",
            "RT @GironAvisai: 🌟NEW GIVEAWAY🌟\n",
            "💰$1.000 Master Card Gif Card💰\n",
            "ENTER Here for FREE👉 https://t.co/S56E774OxS\n",
            "\n",
            "#Giveaway\n",
            "『 ECHO 』\n",
            "\n",
            "難易度：MASTER\n",
            "\n",
            "#プロセカ 曲ボタン\n",
            "#みんなのボタンメーカー\n",
            " https://t.co/0xx9gYVgmJ\n",
            "\n",
            "おー！好きなやつにゃw\n",
            "飯食ってらやってくるにゃ〜\n",
            "@Mastercard \n",
            "You just rock \n",
            "😂😂😂😂\n",
            "RT @thampimuk: Spr Mame ♥️ @Iam_Jathu 💥🔥👌🏼\n",
            "\n",
            "1 Year Of Blockbuster #Master 🔥😎\n",
            "Design By @Iam_Jathu @Team_CDT \n",
            "#Beast @actorvijay https://t.c…\n",
            "@marciogabr_ @KillgoreSfc @DoLocadora @CarlosS93132087 @UOL pior é que deve ser um bundão master na “vida real”, da… https://t.co/xaOJQ0NBPM\n",
            "RT @stussyjimin: jimin: “#HARU he was a rock… then that rock turned human. he has the character that has the qualities of a master” https:/…\n",
            "Money makes a good servant, but a bad master.\n",
            "\n",
            "PUBLIC WINNER UMAR RIAZ\n",
            "\n",
            "#UmarRiaz\n",
            "RT @imjadeja: @KKRiders Its not a master stroke!Just a show off🤣\n",
            "@KatjaGrace For me brain fog has looked like sensory overload. I can only do one thing at a time (which was not me… https://t.co/jejFFQK1an\n",
            "Excited and happy for her\n",
            "RT @Yuzu_Swap: 🍊👉Kindly check these guides, In case you are experiencing any issues😉\n",
            "\n",
            "✅How to connect your Metamask to Oasis Emerald Mainne…\n",
            "RT @Julian02850161: 1/09/2022 Miles Guo: Zeng Qinghong is the real master of the CCP’s politics.\n",
            "\n",
            "1/09/2022 文贵直播：曾庆红是中共的政治高手，他对中国的地理人文了如指掌，…\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "RT @kyriscael: Master Chief?\n",
            "\n",
            "You mind telling me what you were doing at Magfest? https://t.co/wExnQVFNeB\n",
            "RT @PoojaSindhwani: #MondayMotivation\n",
            "God Kaal,\n",
            "who is the master of twenty one brahmands, has pledged that he will not appear before any o…\n",
            "RT @BeastFilmOffl: #Master Thalapathy Unseen Pic 💥✨ #Beast @actorvijay https://t.co/r0lOyCXrfc\n",
            "RT @Raven_art9: Puppet master \n",
            "Available on \n",
            "@opensea \n",
            "1/1\n",
            "#NFTCommmunity #NFTs #NFT #NFTMarketplace #Ethereum \n",
            "\n",
            "Check out this item on Ope…\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "Standing alone at the end of the alley you see a  24 y/o Brass Dragonborn Weapon Master Fighter\n",
            "RT @DrJennyWilson: Please RT! We're hiring a technician to help with insect colonies and experiments with insect-transmitted plant viruses…\n",
            "RT @VST_Usilampatti: Intha Pongal Nammaku Super Collection ma🙌😍\n",
            "\n",
            "#Beast #BeastFirstSingle #Master  #MasterOnSunTv @actorvijay #VijaySocialT…\n",
            "RT @igtamil: Vaathi raidu 🔥💥\n",
            "\n",
            "#ThalapathyVijay #Thalapathy #Vijay #Master #VaathiRaidu #vaathicoming #TamilCinema https://t.co/hmKV1LLbye\n",
            "RT @nunudanaa: kihyun said when he first joined the company he found it very difficult for him to learn choreographies and he just couldn't…\n",
            "RT @BeastFilmOffl: #Master Thalapathy Unseen Pic 💥✨ #Beast @actorvijay https://t.co/r0lOyCXrfc\n",
            "@holochronicles They look of cheaper build than the Master Replica ones but I'd have to see one to really know.\n",
            "@PvanHouwelingen @DeBlauwePen @SCPonderzoek @BNR @Lisawesterveld Oversterfte, Here now Netherlands:Sweden 2021. No… https://t.co/8ba7FHU21F\n",
            "@CryptoTownEU @TamilCryptoSchl \n",
            "@eusdnetwork\n",
            "RT @BhuppsBi: @VootSelect King master\n",
            "YOUTH ICON KARAN \n",
            "#KaranIsTheBoss #KaranKundrra \n",
            "@justvoot\n",
            " \n",
            "@VootSelect\n",
            " \n",
            "@OrmaxMedia\n",
            "RT @Danieljhonnyof1: Master or slave 😈 😇 https://t.co/fkcCcZ5zqH https://t.co/88ppaZtrjt\n",
            "RT @misterminshu: @staceyabrams We need 50 Stacey Abrams one per each state..\n",
            "We need an STACEY ABRAMS MASTER CLASS\n",
            "We need help in Florida…\n",
            "RT @DHFV_Off: Thalapathy Vijay's #Master amazed ₹254.55 Crore worldwide gross and ranks 3rd biggest Indian Grosser globally in 2021 😎 @Beas…\n",
            "RT @TeaPainUSA: All over America tonight, people that think they’re part of a master race are drinkin’ their own pee.\n",
            "@THE_MAGNATE Perfect lesson from a Chart Master &amp; you are right lately we are seeing ,,BIG,, accounts charting 3-5D… https://t.co/sgt8yHGXy7\n",
            "RT @KKRiders: That moment when a classic move in Test cricket actually reminds you of a T20 master stroke! \n",
            "\n",
            "#Ashes #KKR #AmiKKR #AUSvENG h…\n",
            "RT @SomPundit: Ironically, these NGO hacks are mad at these gentlemen for overshadowing their white master Matt Bryden whom they have no qu…\n",
            "I've just been informed that some segment of the right wing has come to the conclusion that Biden is behind the JFK… https://t.co/IqRnzjdLnB\n",
            "RT @Ay_bkini: LOOL! \n",
            "\n",
            "I remember when we said Ambode wasn’t good enough, they disagreed. \n",
            "\n",
            "Few months later, they started claiming Ambode i…\n",
            "RT @insideosunstate: #ForABetterOsun \n",
            "\n",
            "OSUN TOURISM: OYETOLA UNVEILS MASTER PLAN, HONOURS STAKEHOLDERS\n",
            "\n",
            "https://t.co/cGGlyWy9Zr /1 https://…\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongside…\n",
            "RT @miniminicult: haru, jimin’s character, is an immortal guardian who used to be a rock &amp; has the qualities of a master 🤯 https://t.co/qSe…\n",
            "RT @ChennaiTimesTOI: #Kaithi and #Master actor #ArjunDas elated about his parents meeting actor #Suriya, who they admire a lot!\n",
            "\n",
            "https://t.…\n",
            "Well, that's another book full of Old Master drawings on its way...\n",
            "\n",
            "#BookAddict #Art #Books #Bibliophile\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "RT @insideosunstate: Osun State government has set the pace for its tourism development with Governor Adegboyega Oyetola disclosing that th…\n",
            "RT @aeiounis_: The power of du'a masyaAllah 🥺🥰💖✨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, the…\n",
            "@yuzi_chahal https://t.co/pL9TGKPbFy\n",
            "🙏🏻⚡️✨\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongside…\n",
            "Dawg she was NICE with it too like I had to Google if she was actually blind lol\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "RT @JiminGlobal: 7FATES: CHAKHO Interview | Jimin\n",
            "\n",
            "Jimin tells us more about his character, Haru, and how he has the qualities of a master.…\n",
            "RT @jboxbr: Games | ‘Yu-Gi-Oh! Master Duel’ terá versão em português, segundo site oficial. O vindouro game será a versão oficial digital (…\n",
            "Surely, we, the pioneers of science, can use it for good. We're the champions of discovery. Why fear it when we can master it?\n",
            "ithulam eppo nadakumo namma life la 🥲\n",
            "\n",
            "@actorvijay @Samanthaprabhu2 #beast #theri #master #Samantha #BeastFromApril https://t.co/7e0roLQLvl\n",
            "RT @PamplingNews: Haz RT y síguenos, sorteamos 15 camisetas cada mes.\n",
            "🆕 Master Chief ¡solo 10€ durante 24H!\n",
            "Consíguela ya 👉 https://t.co/3R…\n",
            "RT @aeiounis_: The power of du'a masyaAllah 🥺🥰💖✨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, the…\n",
            "RT @ChennaiTimesTOI: #Kaithi and #Master actor #ArjunDas elated about his parents meeting actor #Suriya, who they admire a lot!\n",
            "\n",
            "https://t.…\n",
            "Como que a pessoa te jura amor e depois volta pra ex? GENTE, falsidade nível master.\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "クラロワ障㊙️者ランキング♿️\n",
            "1位G master しおん\n",
            "2位りんご坊や\n",
            "3位トレーナーウルフ\n",
            "#MOONGLOWPHUPDATES\n",
            "\n",
            "MASS UPDATES AS OF 01.11.2022\n",
            "\n",
            "Master Tracker: https://t.co/rbQttbwcbo\n",
            "\n",
            "For questions, don’t he… https://t.co/WzPjIYMqKu\n",
            "I am the least adept with a bow. And that is precisely why I must master it.\n",
            "@CHARANJITCHANNI You have now become a master politician.  you are better actor than @ArvindKejriwal .  Great performance.\n",
            "RT @Iam_Jathu: Here's the Special Design  1 Years of  Block Buster  #Master  @Team_CDT ! ❤ \n",
            "@actorvijay #Beast  \n",
            "#1yearsofBlockBusterMaster…\n",
            "RT @KKRiders: That moment when a classic move in Test cricket actually reminds you of a T20 master stroke! \n",
            "\n",
            "#Ashes #KKR #AmiKKR #AUSvENG h…\n",
            "RT @Vijaysubbu1: If your boss 56\" can pose for camera doing meditation with his spectacles inside the cave, this is very much possible. Can…\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongside…\n",
            "@Apple__iTV\n",
            "@zhitnikoleks @amalini2013\n",
            "RT @Velu_me: #BeastFirstSingle Trending 18.5K Tweets 😳🔥\n",
            "#Master  @actorvijay #Thalapathy66 #Beast\n",
            "@INCIndia https://t.co/pL9TGKPbFy\n",
            "エキスパートだけどフルコン出来たの‼️\n",
            "MASTERは、無理そう（笑） https://t.co/wpmXZbU0Fx\n",
            "Lan SiZhui tried to contain himself, but his voice sounded delighted, “Young Master Mo, you are also here? Then is… https://t.co/PxFGpqcwCR\n",
            "RT @aeiounis_: The power of du'a masyaAllah 🥺🥰💖✨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, the…\n",
            "Starting Your Own Business? Here’s The Top Seven Skills You Need To Master https://t.co/DgPkmsTVxY\n",
            "Regarder le film original, une master class infos sûres\n",
            "#CIO as business partner: 4 prerequisites IT leaders must master\n",
            "\n",
            "https://t.co/Fkxvh4SGGo\n",
            "\n",
            "Basically, ability to ex… https://t.co/UHNysbEWyj\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "Warm, real, or just a cold and lonely, lovely work of art? After 500 years, the Mona Lisa still incites criminal pa… https://t.co/w71JMsJzt5\n",
            "RT @PornKiwi: Part 11 of 69 ! \n",
            "\n",
            " Full video here (Title: Tom of Finland: Master Cut): https://t.co/iNehDwstb4 \n",
            "\n",
            " Follow @NewPornKiwi, @Porn…\n",
            "RT @AcademyOfFags: Please, Master... Please...\n",
            "\n",
            "[Follow backup page @academyoffags2] https://t.co/yOKZAEqDho\n",
            "Pas Naik Angkot Di halo mister wer ar yu going-in\n",
            "\n",
            "\"Bade Ka cijerah, A. Tiasa?\"\n",
            "\n",
            "Bapaknya orang Garut kuliah amerik… https://t.co/X3XR8TsJ90\n",
            "Master Swim Club Tuesdays &amp; Thursdays 5:30-7 p.m.\n",
            "Starts today! Fees: AD, RET, RES, DEP (18+) free; DoD civilians -… https://t.co/bZ9wjokFfm\n",
            "RT @constads: - Τι σε φέρνει στην ελληνική δημοσιογραφία;\n",
            "RT @Team_CDT: 1 Year Of Blockbuster #Master 🔥😎\n",
            "Design By @Iam_Jathu @Team_CDT \n",
            "#Beast @actorvijay https://t.co/CICqmo468R\n",
            "RT @RuazaINC: Prior 2 Corona, should some 1 write even a single sentence/tweet against Bluffs Master Modi, 1000s of Rs 2/tweet Bhakts will…\n",
            "👀\n",
            "RT @AkMaturity: Old @AjithMaturity Sila 🐢gala Report Adichi Suspended Aachi This my new id oru support tweet pottu vidunga 💪\n",
            "\n",
            "#Beast @actor…\n",
            "RT @RuazaINC: उस Bluff Master के \n",
            "कहने को तो 70+ millions followers हैं\n",
            "@Twitter \n",
            "पर कभी किसी ने \n",
            "उन 70+ millions के 1 % यानि \n",
            "700,000 (7 L…\n",
            "#GodiModiDonoFail\n",
            "@GOPLeader Tell that to your master, Putin, you low life traitor!\n",
            "RT @aeiounis_: The power of du'a masyaAllah 🥺🥰💖✨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, the…\n",
            "RT @Team_CDT: 1 Year Of Blockbuster #Master 🔥😎\n",
            "Design By @Iam_Jathu @Team_CDT \n",
            "#Beast @actorvijay https://t.co/CICqmo468R\n",
            "RT @LuvDatta_INC: In Political Science we studied that a bunch of Ministers Sail together &amp; Sink together. Nowadays Godi Media too is inclu…\n",
            "RT @dairingtia: elle a été virée parce qu’elle refusait les avances de son responsable donc PLEASE, si vous avez un contact pour elle :\n",
            "RT @stussyjimin: jimin: “#HARU he was a rock… then that rock turned human. he has the character that has the qualities of a master” https:/…\n",
            "RT @Simple_Livinger: Today's #GodMorningTuesday\n",
            "\n",
            "SUPREME GOD\n",
            "\n",
            "In all our religious texts and scriptures, the glory of that one Prabhu/Maste…\n",
            "In case anyone else feels like skipping today, don't forget these  tips from the master.\n",
            "\n",
            "\"It's a little childish a… https://t.co/1fcXXD0Uzw\n",
            "RT @RuazaINC: Well Said\n",
            "He turned out to be biggest Bluffs Master of the Nation and Liar.\n",
            "@swachhhyd https://t.co/pL9TGKPbFy\n",
            "RT @SEUNGM1NE: hyunjin : “u look like a young master”\n",
            "also hyunjin : “a mindless young master”\n",
            "\n",
            "seungmin was ready to fight😆😆😆😆 https://t.c…\n",
            "RT @Williamz902: I can't help feel that this is a sinful waste of money.\n",
            "\n",
            "https://t.co/SlM2KH84KM\n",
            "RT @PamplingNews: Haz RT y síguenos, sorteamos 15 camisetas cada mes.\n",
            "🆕 Master Chief ¡solo 10€ durante 24H!\n",
            "Consíguela ya 👉 https://t.co/3R…\n",
            "Jveux monter ce compte master si quelqu'un a un compte à monter aussi et veut duo go dm \n",
            "\n",
            "https://t.co/Y0LRLAYJeS\n",
            "RT @frhnaism: Pertengahan tahun lepas i prayed “ya Allah, permudahkan urusan jodoh aku. kalau sampai umur 28 aku tak kahwin, aku memang tak…\n",
            "RT @TamilVi17140583: 🤣🤣🤣🤣\n",
            "RT @FranQAnde: Fortunately and unfortunately to make Nigeria work we mostly have to be like Tunde the chess master, one by one we must fix…\n",
            "RT @13phiIe: haru has the qualities of a master !  https://t.co/uKci80H3Uw\n",
            "RT @nunudanaa: kihyun said when he first joined the company he found it very difficult for him to learn choreographies and he just couldn't…\n",
            "It was inevitable.  Their Fuhrer drinks pee, Russian hooker pee.  Best in the world!  Produce of champagne and cavi… https://t.co/QKgzAwzKv1\n",
            "How do you focus and master your vision in life?\n",
            "\n",
            "(Source:  E/a)\n",
            "\n",
            "For ideas, try:\n",
            "\n",
            "- Law of Success book - Hill\n",
            "- T… https://t.co/2Y2HXyLXrq\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "RT @banzai_TRPG: 💫 세베루스 스네이프, 호그와트에 돌아온 것을 환영합니다.\n",
            "RT @nunudanaa: kihyun said when he first joined the company he found it very difficult for him to learn choreographies and he just couldn't…\n",
            "RT @WasimJaffer14: Captain, Opener, No.3, Wicket Keeper, ICC event winning coach, former NCA head, and now India coach. But above all a gre…\n",
            "RT @moonglowphCS: #MOONGLOWPHUPDATES\n",
            "\n",
            "MASS UPDATES AS OF 01.11.2022\n",
            "\n",
            "Master Tracker: https://t.co/rbQttbwcbo\n",
            "\n",
            "For questions, don’t hesitate…\n",
            "@iams_eth master GIF\n",
            "RT @KAILASA_UN: \"Order is chaos.\n",
            "There is a beautiful Zen story: \n",
            "A disciple goes to a Zen master and asks, \n",
            "‟Master, please explain to me…\n",
            "\"We cannot always control our thoughts, but we can control our words, and repetition impresses the subconscious, an… https://t.co/NMVL8egZoZ\n",
            "RT @ammisays: i don’t know whether the idea that jimin is a master of witchcraft or that he’s a powerful shapeshifter makes me more excited…\n",
            "RT @UDCEHD: What is it like to be an education or human development graduate student at UD? Students from our Master's and Ed.D. programs w…\n",
            "『 独りんぼエンヴィー 』\n",
            "\n",
            "難易度：MASTER\n",
            "嘘wすごい速さで終わるんだけどw明日やるわ\n",
            "#プロセカ 曲ボタン\n",
            "#みんなのボタンメーカー\n",
            " https://t.co/dPlTqBdaCf\n",
            "RT @Sissy_Trainers: If Your Master Give You Two Option For Sissygasm\n",
            "1-CuM Through AnaL By DilDo Or\n",
            "2-CuM Through AnaL By His CocK \n",
            "&amp; Ask U…\n",
            "@ZakaWaqar @ImranKhanPTI #ImranKhanPTI respected sir please hat jor k Ap say request ha main ak mother hu mari job… https://t.co/98FjFEidod\n",
            "RT @GersonKoringa: \"Manchéster unaiti\"\n",
            "\"Master Chef United\n",
            "\"Manchester Unlait\"\n",
            "\n",
            "Simplesmente El gordo Braz poliglota kkkkkkkkk https://t.co…\n",
            "RT @HayateManazili: 【MAD】初代ARMORED CORE＆ARMORED CORE MASTER OF ARENA https://t.co/wfsfJRBaql #sm249748 #ニコニコ動画\n",
            "この米好き https://t.co/oiJiGRNp5x\n",
            "RT @imas_columbia: 「星環世界」明日発売🚀💫\n",
            "\n",
            "新シリーズ「THE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 星環世界」\n",
            "\n",
            "Starring by\n",
            "#砂塚あきら #速水奏 #高垣楓 #…\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongside…\n",
            "RT @Sissy_Trainers: Retweet If You Love To Watch Or Want To Watch Your Clitty Flip Flop In The Mirror When You Ride On A CocK Or On A DilDo…\n",
            "RT @aeiounis_: The power of du'a masyaAllah 🥺🥰💖✨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, the…\n",
            "RT @Mkumari99: #GodmorningMonday\n",
            "Sant Rampal Ji Maharaj is the only spiritual master in the world who has the right to conduct satsang and…\n",
            "Twitter is a great place for secrets, right? I'm sure I can trust you with this one... 🤫\n",
            "\n",
            "https://t.co/O4esQdpwFq\n",
            "RT @wimipolis: Ich: 39, örtlich flexibel, Master Militärgeschichte/Militärsoziologie, fast 20 Jahre Arbeitserfahrung in Wissenschaft, Polit…\n",
            "RT @NewPornKiwi: Part 17 of 69 ! \n",
            "\n",
            " Full video here (Title: Tom of Finland: Master Cut): https://t.co/IrauEVCV9H \n",
            "\n",
            " Follow @NewPornKiwi, @P…\n",
            "@1970euro @robfoot And not even a librarian. He teaches at the school of library science at U of T. Teaches. A Mast… https://t.co/oTjfUesy3P\n",
            "I want to master two lyrical things in working on and strengthen my vocal cords so I can have breathing control whi… https://t.co/x7xgPI675S\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-9bde13b980d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtwitterStream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtwitterStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"master\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filter_level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delimited'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, is_async)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[0;34m(self, resp)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mstripped_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mline\u001b[0m \u001b[0;31m# line is sometimes None so we need to check here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstripped_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Read the next chunk size from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBBombq6GgV"
      },
      "source": [
        "# from tweepy import Stream\n",
        "# from tweepy import OAuthHandler\n",
        "# from tweepy.streaming import StreamListener\n",
        "# import json\n",
        "# import sent_mod1 as s\n",
        "\n",
        "# #consumer key, consumer secret, access token, access secret.\n",
        "# ckey=\"XXeG5cy7QUfahiOHlacBaddIF\"\n",
        "# csecret=\"quSrqvVZUfRqbp8SZ0bDZimz8avlS96htsonlG1dFAehgsmMI1\"\n",
        "# atoken=\"1399846090699067392-W14id1VCFiLIib3MD7kaT2C6pvDmvX\"\n",
        "# asecret=\"6nrOLFSELerY9HzGSV5YGNigZnhSM54izskj2iahnFina\"\n",
        "\n",
        "# #from twitterapistuff import *\n",
        "\n",
        "# class listener(StreamListener):\n",
        "#   def on_data(self, data):\n",
        "#     all_data = json.loads(data)\n",
        "#     tweet = all_data[\"text\"]\n",
        "#     sentiment_value, confidence = s.sentiment(tweet)\n",
        "#     print(tweet, sentiment_value)\n",
        "#     if sentiment_value==\"pos\":\n",
        "#       output = open(\"twitter_pos_out.txt\",\"a\")\n",
        "#       output.write(tweet)\n",
        "#       output.write('\\n')\n",
        "#       output.close()\n",
        "#     else:\n",
        "#       output = open(\"twitter_neg_out.txt\",\"a\")\n",
        "#       output.write(tweet)\n",
        "#       output.write('\\n')\n",
        "#       output.close()\n",
        "#     return True\n",
        "\n",
        "#   def on_error(self, status):\n",
        "#     print(status)\n",
        "\n",
        "# auth = OAuthHandler(ckey, csecret)\n",
        "# auth.set_access_token(atoken, asecret)\n",
        "\n",
        "# twitterStream = Stream(auth, listener())\n",
        "# twitterStream.filter(track=[\"rajini\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyf5XdENdnYo"
      },
      "source": [
        "Code switching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkNlDTglhMzS",
        "outputId": "b6205fcf-3385-417b-ab25-a44ed1c6cef5"
      },
      "source": [
        "pip install langdetect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\r\u001b[K     |▍                               | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 25.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40kB 18.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51kB 16.0MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71kB 13.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81kB 14.7MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 14.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102kB 10.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112kB 10.6MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 10.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133kB 10.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 174kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 194kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 204kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 225kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 235kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 256kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 266kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 286kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 296kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 317kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 327kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 348kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 358kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 378kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 389kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 409kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 419kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 440kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 450kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 471kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 481kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 501kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 512kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 532kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 542kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 563kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 573kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 593kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 604kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 624kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 634kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 655kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 665kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 686kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 696kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 716kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 727kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 747kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 757kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 768kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 778kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 788kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 798kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 808kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 819kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 829kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 839kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 849kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 860kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 870kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 880kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 890kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 901kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 911kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 921kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 931kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 942kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 952kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 962kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 972kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 983kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993242 sha256=3c4dda5642c801ee2d5aa6f5f005ed9c3da8bd0ee528022aefad2e63fd192581\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xbOiswdhT6H",
        "outputId": "3e05d32f-a945-47d2-8d77-44d7733d4841"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.9MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/b5/96947391e84e332010694ac8c4ba64841b90301ee0d05e7236ff934bb9f6/hstspreload-2021.7.5-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 22.1MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.5.30)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=b539242428d4bf29b20e00e45f6f6cc949100996192f58a8ddf065083a164be7\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hstspreload, rfc3986, sniffio, h11, hpack, hyperframe, h2, httpcore, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.7.5 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBTm0u9K9U6k"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLG1jhNM9ZCF"
      },
      "source": [
        "LinearSVC_classifier = SklearnClassifier(LinearSVC())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u1Um1gp9ziM"
      },
      "source": [
        "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dho5_nQm-MUO"
      },
      "source": [
        "SGDC_classifier = SklearnClassifier(SGDClassifier())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sME2qeJzhbln"
      },
      "source": [
        "dataset = pd.read_excel('/content/drive/MyDrive/Research/NLTK VIthu (2).xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWNGB4Alh6G8"
      },
      "source": [
        "#Cleaning the text\n",
        "def tokenizer(sentence):\n",
        "    mytokens=word_tokenize(sentence)\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbcUiSAvh9at"
      },
      "source": [
        "#Cleaning text\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        return [clean_text(text) for text in X]\n",
        "    def fit(self, X, y, **fit_params):\n",
        "        return self\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text \n",
        "def clean_text(text):     \n",
        "    return text.strip().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmiPRvQpiBE6"
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,1)) \n",
        "tfvectorizer = TfidfVectorizer(tokenizer = tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsvtGarEiERU"
      },
      "source": [
        "X = dataset['Text']\n",
        "y = dataset['Language']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=77)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LbvpWKiG_Y",
        "outputId": "a28c9f47-e454-4eee-a88a-e97f172e4c24"
      },
      "source": [
        "#Using third model\n",
        "SVCclassifier = LinearSVC()\n",
        "SVCmodel = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', SVCclassifier)])\n",
        "\n",
        "# Train the Model\n",
        "SVCmodel.fit(X_train,y_train)   \n",
        "pred3= SVCmodel.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[296   1   0]\n",
            " [  0 309   0]\n",
            " [  4  11 279]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       0.99      1.00      0.99       297\n",
            "       Tamil       0.96      1.00      0.98       309\n",
            "   Thanglish       1.00      0.95      0.97       294\n",
            "\n",
            "    accuracy                           0.98       900\n",
            "   macro avg       0.98      0.98      0.98       900\n",
            "weighted avg       0.98      0.98      0.98       900\n",
            "\n",
            "Accuracy: 98.22222222222223%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm6OfsdbASRz",
        "outputId": "ed30acff-3b2c-4264-d54b-6ee1e805440d"
      },
      "source": [
        "BernoulliNB = BernoulliNB()\n",
        "BernoulliNB_model = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', BernoulliNB)])\n",
        "\n",
        "# Train the Model\n",
        "BernoulliNB_model.fit(X_train,y_train)   \n",
        "pred3= BernoulliNB_model.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[265   3  29]\n",
            " [  0 309   0]\n",
            " [  0   2 292]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       1.00      0.89      0.94       297\n",
            "       Tamil       0.98      1.00      0.99       309\n",
            "   Thanglish       0.91      0.99      0.95       294\n",
            "\n",
            "    accuracy                           0.96       900\n",
            "   macro avg       0.96      0.96      0.96       900\n",
            "weighted avg       0.97      0.96      0.96       900\n",
            "\n",
            "Accuracy: 96.22222222222221%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmecjcFA7TVu",
        "outputId": "d3e54da5-12e2-4439-abf2-0f1433b7a96c"
      },
      "source": [
        "logesticRegression = LogisticRegression()\n",
        "logisticModel = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', logesticRegression)])\n",
        "# Train the Model\n",
        "logisticModel.fit(X_train,y_train)   \n",
        "pred3= logisticModel.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[291   4   2]\n",
            " [  0 308   1]\n",
            " [  4  13 277]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       0.99      0.98      0.98       297\n",
            "       Tamil       0.95      1.00      0.97       309\n",
            "   Thanglish       0.99      0.94      0.97       294\n",
            "\n",
            "    accuracy                           0.97       900\n",
            "   macro avg       0.97      0.97      0.97       900\n",
            "weighted avg       0.97      0.97      0.97       900\n",
            "\n",
            "Accuracy: 97.33333333333334%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOZ7vCR1-P1u",
        "outputId": "ba99fed3-4ad3-41ea-8340-cde8d0fce6d7"
      },
      "source": [
        "SGDClassifier = SGDClassifier()\n",
        "SGDV_Model = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', SGDClassifier)])\n",
        "# Train the Model\n",
        "SGDV_Model.fit(X_train,y_train)   \n",
        "pred3= SGDV_Model.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[294   2   1]\n",
            " [  0 308   1]\n",
            " [  3  11 280]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       0.99      0.99      0.99       297\n",
            "       Tamil       0.96      1.00      0.98       309\n",
            "   Thanglish       0.99      0.95      0.97       294\n",
            "\n",
            "    accuracy                           0.98       900\n",
            "   macro avg       0.98      0.98      0.98       900\n",
            "weighted avg       0.98      0.98      0.98       900\n",
            "\n",
            "Accuracy: 98.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vmku1hbUi6L",
        "outputId": "c6dccdba-4ae8-4d75-a750-21d9ac81de6d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD7zkIu1iKxe",
        "outputId": "62ef0f97-57bb-49b2-ae37-7d63b0c2b162"
      },
      "source": [
        "pre = SVCmodel.predict([\"யாராவது முழு சொற்றொடரையும் மீண்டும் மீண்டும் செய்ய விரும்பினால் கடைசி வார்த்தையை நான் பிடிக்கவில்லை என்பதால் நீங்கள் அதை மீண்டும் செய்ய முடியுமா?\"])\n",
        "print(pre[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamil\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF-K9gwH_6wg",
        "outputId": "e41fb3c5-bc83-448f-fbcd-e33067f35ec2"
      },
      "source": [
        "pre = SGDV_Model.predict([\"யாராவது முழு சொற்றொடரையும் மீண்டும் மீண்டும் செய்ய விரும்பினால் கடைசி வார்த்தையை நான் பிடிக்கவில்லை என்பதால் நீங்கள் அதை மீண்டும் செய்ய முடியுமா?\"])\n",
        "print(pre[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xlOzNkHiP6S"
      },
      "source": [
        "t1=\"பத்த வச்ச நெருப்பு பரபரன்னு பற்றி எரிவது போல படம் ஓடுகிறது.. தொடக்கத்திலிருந்து இறுதி வரை.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Ig2vXjiSdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4756ec-da6c-4d73-f6d6-745b0cd472ea"
      },
      "source": [
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "print(detect(t1))\n",
        "translator= Translator()\n",
        "translatedSent= translator.translate(t1, src='ta', dest='en',)\n",
        "print(translatedSent.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ta\n",
            "The film runs like wildfire about the Patha Vachcha fire sensation .. from beginning to end.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uovC4Xfoe8Wi"
      },
      "source": [
        "Language detection and translation normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIAqqMCKiWUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c52f937-f548-4146-93f8-3c38bab98471"
      },
      "source": [
        "#from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "tweet_try=[\"vanakam thala padam vera level la irunthathu msg \"]\n",
        "\n",
        "\n",
        "pre = SVCmodel.predict(tweet_try)\n",
        "print(pre[0])\n",
        "if pre[0]==\"English\":\n",
        "  print(tweet_try[0])\n",
        "elif pre[0]==\"Thanglish\":\n",
        "  translator1= Translator()\n",
        "  translatedSent= translator1.translate(tweet_try[0], src='ta', dest='en')\n",
        "  print(translatedSent.text)  \n",
        "else:\n",
        "  translator= Translator()\n",
        "  translatedSent= translator.translate(tweet_try[0],src='ta',dest='en')\n",
        "  print(translatedSent.text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanglish\n",
            "Hello head movie was on another level\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NztT7eere5FI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j35mMDVlg492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "4ae2921e-c087-4f8e-be57-05b3d469bf92"
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import sent_mod1 as s\n",
        "\n",
        "#consumer key, consumer secret, access token, access secret.\n",
        "ckey=\"XXeG5cy7QUfahiOHlacBaddIF\"\n",
        "csecret=\"quSrqvVZUfRqbp8SZ0bDZimz8avlS96htsonlG1dFAehgsmMI1\"\n",
        "atoken=\"1399846090699067392-W14id1VCFiLIib3MD7kaT2C6pvDmvX\"\n",
        "asecret=\"6nrOLFSELerY9HzGSV5YGNigZnhSM54izskj2iahnFina\"\n",
        "\n",
        "#from twitterapistuff import *\n",
        "\n",
        "class listener(StreamListener):\n",
        "  def on_data(self, data):\n",
        "    all_data = json.loads(data)\n",
        "    tweet = all_data[\"text\"]\n",
        "    tweet_try=[tweet]\n",
        "    pre = SVCmodel.predict(tweet_try)\n",
        "    if pre[0]==\"English\":\n",
        "      sentiment_value, confidence = s.sentiment(tweet)\n",
        "    elif pre[0]==\"Thanglish\":\n",
        "      translator1= Translator()\n",
        "      translatedSent= translator1.translate(tweet_try[0], src='ta', dest='en')\n",
        "      sentiment_value, confidence = s.sentiment('translatedSent')  \n",
        "    else:\n",
        "      translator= Translator()\n",
        "      translatedSent= translator.translate(tweet_try[0], src='ta', dest='en')\n",
        "      sentiment_value, confidence = s.sentiment('translatedSent')\n",
        "    # sentiment_value, confidence = s.sentiment(tweet)\n",
        "    print(tweet, sentiment_value)\n",
        "    if sentiment_value==\"pos\":\n",
        "      output = open(\"twitter_pos_out.txt\",\"a\")\n",
        "      output.write(tweet)\n",
        "      output.write('\\n')\n",
        "      output.close()\n",
        "    else:\n",
        "      output = open(\"twitter_neg_out.txt\",\"a\")\n",
        "      output.write(tweet)\n",
        "      output.write('\\n')\n",
        "      output.close()\n",
        "    return True\n",
        "\n",
        "  def on_error(self, status):\n",
        "    print(status)\n",
        "\n",
        "auth = OAuthHandler(ckey, csecret)\n",
        "auth.set_access_token(atoken, asecret)\n",
        "\n",
        "twitterStream = Stream(auth, listener())\n",
        "twitterStream.filter(track=[\"valimai\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5be117583b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamListener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msent_mod1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#consumer key, consumer secret, access token, access secret.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sent_mod1'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}