{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vithurshiniS/Twitter-Sentiment-Analysis/blob/main/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAm6N9ypFMJJ",
        "outputId": "1bddb9cb-fa2f-412e-fdbf-1465410bdfd9"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BilQeFzMjH36"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxNxT214cVCb"
      },
      "source": [
        "Language Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uHNjaWGhsOt",
        "outputId": "ca16a424-d94d-4b77-b5ed-08bb9f48a568"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import string\n",
        "import pickle\n",
        "import nltk\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.base import TransformerMixin \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBsLw5-5cauU"
      },
      "source": [
        "Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoSofuSWFQOG",
        "outputId": "d35a7d68-63c7-4999-89b8-442262bfb43e"
      },
      "source": [
        "import nltk\n",
        "import random\n",
        "#from nltk.corpus import movie_reviews\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "class VoteClassifier(ClassifierI):\n",
        "  def __init__(self, *classifiers):\n",
        "    self._classifiers = classifiers\n",
        "\n",
        "  def classify(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    return mode(votes)\n",
        "\n",
        "  def confidence(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "\n",
        "    choice_votes = votes.count(mode(votes))\n",
        "    conf = choice_votes / len(votes)\n",
        "    return conf\n",
        "    \n",
        "short_pos = open(\"/content/drive/MyDrive/Research/positive_en.txt\",\"r\",encoding=\"latin-1\").read()\n",
        "short_neg = open(\"/content/drive/MyDrive/Research/negative_en.txt\",\"r\",encoding=\"latin-1\").read()\n",
        "\n",
        "# move this up here\n",
        "all_words = []\n",
        "documents = []\n",
        "\n",
        "\n",
        "#  j is adject, r is adverb, and v is verb\n",
        "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
        "allowed_word_types = [\"J\"]\n",
        "\n",
        "for p in short_pos.split('\\n'):\n",
        "  documents.append( (p, \"pos\") )\n",
        "  words = word_tokenize(p)\n",
        "  pos = nltk.pos_tag(words)\n",
        "  for w in pos:\n",
        "    if w[1][0] in allowed_word_types:\n",
        "      all_words.append(w[0].lower())\n",
        "\n",
        "    \n",
        "for p in short_neg.split('\\n'):\n",
        "  documents.append( (p, \"neg\") )\n",
        "  words = word_tokenize(p)\n",
        "  pos = nltk.pos_tag(words)\n",
        "  for w in pos:\n",
        "    if w[1][0] in allowed_word_types:\n",
        "      all_words.append(w[0].lower())\n",
        "\n",
        "\n",
        "\n",
        "save_documents = open(\"/content/drive/MyDrive/Research/documents.pickle\",\"wb\")\n",
        "pickle.dump(documents, save_documents)\n",
        "save_documents.close()\n",
        "\n",
        "\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "\n",
        "word_features = list(all_words.keys())[:5000]\n",
        "\n",
        "\n",
        "save_word_features = open(\"/content/drive/MyDrive/Research/word_features5k.pickle\",\"wb\")\n",
        "pickle.dump(word_features, save_word_features)\n",
        "save_word_features.close()\n",
        "\n",
        "\n",
        "def find_features(document):\n",
        "  words = word_tokenize(document)\n",
        "  features = {}\n",
        "  for w in word_features:\n",
        "    features[w] = (w in words)\n",
        "\n",
        "  return features\n",
        "\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
        "\n",
        "random.shuffle(featuresets)\n",
        "print(len(featuresets))\n",
        "\n",
        "testing_set = featuresets[10000:]\n",
        "training_set = featuresets[:10000]\n",
        "\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
        "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
        "classifier.show_most_informative_features(15)\n",
        "\n",
        "###############\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/originalnaivebayes5k.pickle\",\"wb\")\n",
        "pickle.dump(classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
        "MNB_classifier.train(training_set)\n",
        "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/MNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(MNB_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
        "BernoulliNB_classifier.train(training_set)\n",
        "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/BernoulliNB_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
        "LogisticRegression_classifier.train(training_set)\n",
        "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/LogisticRegression_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "\n",
        "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
        "LinearSVC_classifier.train(training_set)\n",
        "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/LinearSVC_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(LinearSVC_classifier, save_classifier)\n",
        "save_classifier.close()\n",
        "\n",
        "\n",
        "##NuSVC_classifier = SklearnClassifier(NuSVC())\n",
        "##NuSVC_classifier.train(training_set)\n",
        "##print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
        "\n",
        "\n",
        "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
        "SGDC_classifier.train(training_set)\n",
        "print(\"SGDClassifier accuracy percent:\",nltk.classify.accuracy(SGDC_classifier, testing_set)*100)\n",
        "\n",
        "save_classifier = open(\"/content/drive/MyDrive/Research/SGDC_classifier5k.pickle\",\"wb\")\n",
        "pickle.dump(SGDC_classifier, save_classifier)\n",
        "save_classifier.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10664\n",
            "Original Naive Bayes Algo accuracy percent: 71.3855421686747\n",
            "Most Informative Features\n",
            "              engrossing = True              pos : neg    =     20.9 : 1.0\n",
            "                  boring = True              neg : pos    =     19.5 : 1.0\n",
            "                   flaws = True              pos : neg    =     16.2 : 1.0\n",
            "                mediocre = True              neg : pos    =     15.7 : 1.0\n",
            "                 routine = True              neg : pos    =     15.1 : 1.0\n",
            "                    flat = True              neg : pos    =     14.7 : 1.0\n",
            "              refreshing = True              pos : neg    =     13.6 : 1.0\n",
            "               inventive = True              pos : neg    =     13.6 : 1.0\n",
            "                    warm = True              pos : neg    =     12.5 : 1.0\n",
            "               wonderful = True              pos : neg    =     12.5 : 1.0\n",
            "                powerful = True              pos : neg    =     12.4 : 1.0\n",
            "                touching = True              pos : neg    =     11.2 : 1.0\n",
            "                mindless = True              neg : pos    =     11.1 : 1.0\n",
            "                   stale = True              neg : pos    =     11.1 : 1.0\n",
            "             mesmerizing = True              pos : neg    =     10.9 : 1.0\n",
            "MNB_classifier accuracy percent: 71.3855421686747\n",
            "BernoulliNB_classifier accuracy percent: 71.6867469879518\n",
            "LogisticRegression_classifier accuracy percent: 71.23493975903614\n",
            "LinearSVC_classifier accuracy percent: 69.27710843373494\n",
            "SGDClassifier accuracy percent: 70.93373493975903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGwD9z7-csnC"
      },
      "source": [
        "Creating sentiment module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GqX6M5tMIHf",
        "outputId": "a22a8a8d-e53d-45db-f7ad-37141ccccfdb"
      },
      "source": [
        "#File: sentiment_mod.py\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "#from nltk.corpus import movie_reviews\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "import pickle\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "class VoteClassifier(ClassifierI):\n",
        "  def __init__(self, *classifiers):\n",
        "    self._classifiers = classifiers\n",
        "\n",
        "  def classify(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "    return mode(votes)\n",
        "\n",
        "  def confidence(self, features):\n",
        "    votes = []\n",
        "    for c in self._classifiers:\n",
        "      v = c.classify(features)\n",
        "      votes.append(v)\n",
        "\n",
        "    choice_votes = votes.count(mode(votes))\n",
        "    conf = choice_votes / len(votes)\n",
        "    return conf\n",
        "\n",
        "\n",
        "documents_f = open(\"/content/drive/MyDrive/Research/documents.pickle\", \"rb\")\n",
        "documents = pickle.load(documents_f)\n",
        "documents_f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_features5k_f = open(\"/content/drive/MyDrive/Research/word_features5k.pickle\", \"rb\")\n",
        "word_features = pickle.load(word_features5k_f)\n",
        "word_features5k_f.close()\n",
        "\n",
        "\n",
        "def find_features(document):\n",
        "  words = word_tokenize(document)\n",
        "  features = {}\n",
        "  for w in word_features:\n",
        "    features[w] = (w in words)\n",
        "\n",
        "  return features\n",
        "\n",
        "\n",
        "\n",
        "# featuresets_f = open(\"/content/drive/MyDrive/Research_2021/featuresets.pickle\", \"rb\")\n",
        "# featuresets = pickle.load(featuresets_f)\n",
        "# featuresets_f.close()\n",
        "\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
        "\n",
        "random.shuffle(featuresets)\n",
        "print(len(featuresets))\n",
        "\n",
        "testing_set = featuresets[10000:]\n",
        "training_set = featuresets[:10000]\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/originalnaivebayes5k.pickle\", \"rb\")\n",
        "classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/MNB_classifier5k.pickle\", \"rb\")\n",
        "MNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/BernoulliNB_classifier5k.pickle\", \"rb\")\n",
        "BernoulliNB_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/LogisticRegression_classifier5k.pickle\", \"rb\")\n",
        "LogisticRegression_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/LinearSVC_classifier5k.pickle\", \"rb\")\n",
        "LinearSVC_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "open_file = open(\"/content/drive/MyDrive/Research/SGDC_classifier5k.pickle\", \"rb\")\n",
        "SGDC_classifier = pickle.load(open_file)\n",
        "open_file.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "voted_classifier = VoteClassifier(classifier,LinearSVC_classifier,MNB_classifier,BernoulliNB_classifier,LogisticRegression_classifier)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sentiment(text):\n",
        "  feats = find_features(text)\n",
        "  return voted_classifier.classify(feats),voted_classifier.confidence(feats)\n",
        "\n",
        "\n",
        "\n",
        "print(sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
        "print(sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10664\n",
            "('pos', 1.0)\n",
            "('neg', 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBhv-4lc7H7"
      },
      "source": [
        "Importing sentiment module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr13im9AEuEb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LvW28JOEt1F"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpSEqRGQSfy",
        "outputId": "282946f8-f036-4c76-970f-9ef359972a6c"
      },
      "source": [
        "!ls /content/drive/MyDrive/Research/sent_mod1.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/sent_mod1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPS0KrBmRBWW"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/Research')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga9bhhlLQmdK",
        "outputId": "255be099-d064-41b0-b892-9a036980d3f0"
      },
      "source": [
        "import sent_mod1 as s\n",
        "\n",
        "print(s.sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))\n",
        "print(s.sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10664\n",
            "('pos', 1.0)\n",
            "('neg', 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8ZdhBGAdFtJ"
      },
      "source": [
        "Sentiment Analysis without language detection\n",
        "Social Media Streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qWupp5aBSkeh",
        "outputId": "fafd101f-cb9f-438d-cc3e-350bd1cc93bf"
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import sent_mod1 as s\n",
        "\n",
        "#consumer key, consumer secret, access token, access secret.\n",
        "ckey=\"xxxxxxxxxxxxxxxxxxxxx\"\n",
        "csecret=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "atoken=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "asecret=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
        "\n",
        "#from twitterapistuff import *\n",
        "\n",
        "class listener(StreamListener):\n",
        "  def on_data(self, data):\n",
        "    all_data = json.loads(data)\n",
        "    tweet = all_data[\"text\"]\n",
        "    sentiment_value, confidence = s.sentiment(tweet)\n",
        "    print(tweet, sentiment_value)\n",
        "\n",
        "\t  # if confidence*100 >= 80:\n",
        "    #   output = open(\"twitter-out.txt\",\"a\")\n",
        "    #   output.write(sentiment_value)\n",
        "    #   output.write('\\n')\n",
        "    #   output.close()\n",
        "\n",
        "      #return True\n",
        "\n",
        "  def on_error(self, status):\n",
        "    print(status)\n",
        "\n",
        "auth = OAuthHandler(ckey, csecret)\n",
        "auth.set_access_token(atoken, asecret)\n",
        "\n",
        "twitterStream = Stream(auth, listener())\n",
        "twitterStream.filter(track=[\"valimai\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongsideâ€¦\n",
            "RT @WasimJaffer14: Captain, Opener, No.3, Wicket Keeper, ICC event winning coach, former NCA head, and now India coach. But above all a greâ€¦\n",
            "RT @FCM_Production: 01 Year of Master ğŸ¥ºğŸ˜\n",
            "@actorvijay #Master \n",
            ".\n",
            "#Beast #BeastFirstSingle \n",
            "#BeastUpdate https://t.co/0Eb9HsCgTI\n",
            "RT @TeaPainUSA: All over America tonight, people that think theyâ€™re part of a master race are drinkinâ€™ their own pee.\n",
            "RT @42M4rvin: Read the rest of the article! https://t.co/YvjA5C8kqd\n",
            "fisting #balls #ballbusting #ballbashing #punch #big\n",
            "#balls #bust #fist #hung #dick #jock #ballslick #ballbusterâ€¦ https://t.co/y7n9NMCNfn\n",
            "RT @TeaPainUSA: All over America tonight, people that think theyâ€™re part of a master race are drinkinâ€™ their own pee.\n",
            "RT @LuvDatta_INC: In Political Science we studied that a bunch of Ministers Sail together &amp; Sink together. Nowadays Godi Media too is incluâ€¦\n",
            "@setsuna_maimai Fragrance Re:MASTER ãƒŸãƒ©ãƒ¼â†º\n",
            "æ‰¿è«¾ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™â€¦ï¼ï¼\n",
            "RT @wizardingworld: Happy Birthday to Hogwarts Potions master, Severus Snape ğŸ https://t.co/gCefVS6pyo\n",
            "I love this letter... and this website @facinghistory Letter from Jourdon Anderson: A Freedman Writes His Former Maâ€¦ https://t.co/IHxY4FsmKu\n",
            "@manuelaxntj Selo gada master\n",
            "RT @lovelySHEIK3: Hiii friends I AM BACK ğŸ˜…ğŸ˜\n",
            "#master #Beast @actorvijay https://t.co/YwdG8sGmnK\n",
            "RT @GironAvisai: ğŸŒŸNEW GIVEAWAYğŸŒŸ\n",
            "ğŸ’°$1.000 Master Card Gif CardğŸ’°\n",
            "ENTER Here for FREEğŸ‘‰ https://t.co/S56E774OxS\n",
            "\n",
            "#Giveaway\n",
            "ã€ ECHO ã€\n",
            "\n",
            "é›£æ˜“åº¦ï¼šMASTER\n",
            "\n",
            "#ãƒ—ãƒ­ã‚»ã‚« æ›²ãƒœã‚¿ãƒ³\n",
            "#ã¿ã‚“ãªã®ãƒœã‚¿ãƒ³ãƒ¡ãƒ¼ã‚«ãƒ¼\n",
            " https://t.co/0xx9gYVgmJ\n",
            "\n",
            "ãŠãƒ¼ï¼å¥½ããªã‚„ã¤ã«ã‚ƒw\n",
            "é£¯é£Ÿã£ã¦ã‚‰ã‚„ã£ã¦ãã‚‹ã«ã‚ƒã€œ\n",
            "@Mastercard \n",
            "You just rock \n",
            "ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\n",
            "RT @thampimuk: Spr Mame â™¥ï¸ @Iam_Jathu ğŸ’¥ğŸ”¥ğŸ‘ŒğŸ¼\n",
            "\n",
            "1 Year Of Blockbuster #Master ğŸ”¥ğŸ˜\n",
            "Design By @Iam_Jathu @Team_CDT \n",
            "#Beast @actorvijay https://t.câ€¦\n",
            "@marciogabr_ @KillgoreSfc @DoLocadora @CarlosS93132087 @UOL pior Ã© que deve ser um bundÃ£o master na â€œvida realâ€, daâ€¦ https://t.co/xaOJQ0NBPM\n",
            "RT @stussyjimin: jimin: â€œ#HARU he was a rockâ€¦ then that rock turned human. he has the character that has the qualities of a masterâ€ https:/â€¦\n",
            "Money makes a good servant, but a bad master.\n",
            "\n",
            "PUBLIC WINNER UMAR RIAZ\n",
            "\n",
            "#UmarRiaz\n",
            "RT @imjadeja: @KKRiders Its not a master stroke!Just a show offğŸ¤£\n",
            "@KatjaGrace For me brain fog has looked like sensory overload. I can only do one thing at a time (which was not meâ€¦ https://t.co/jejFFQK1an\n",
            "Excited and happy for her\n",
            "RT @Yuzu_Swap: ğŸŠğŸ‘‰Kindly check these guides, In case you are experiencing any issuesğŸ˜‰\n",
            "\n",
            "âœ…How to connect your Metamask to Oasis Emerald Mainneâ€¦\n",
            "RT @Julian02850161: 1/09/2022 Miles Guo: Zeng Qinghong is the real master of the CCPâ€™s politics.\n",
            "\n",
            "1/09/2022 æ–‡è´µç›´æ’­ï¼šæ›¾åº†çº¢æ˜¯ä¸­å…±çš„æ”¿æ²»é«˜æ‰‹ï¼Œä»–å¯¹ä¸­å›½çš„åœ°ç†äººæ–‡äº†å¦‚æŒ‡æŒï¼Œâ€¦\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "RT @kyriscael: Master Chief?\n",
            "\n",
            "You mind telling me what you were doing at Magfest? https://t.co/wExnQVFNeB\n",
            "RT @PoojaSindhwani: #MondayMotivation\n",
            "God Kaal,\n",
            "who is the master of twenty one brahmands, has pledged that he will not appear before any oâ€¦\n",
            "RT @BeastFilmOffl: #Master Thalapathy Unseen Pic ğŸ’¥âœ¨ #Beast @actorvijay https://t.co/r0lOyCXrfc\n",
            "RT @Raven_art9: Puppet master \n",
            "Available on \n",
            "@opensea \n",
            "1/1\n",
            "#NFTCommmunity #NFTs #NFT #NFTMarketplace #Ethereum \n",
            "\n",
            "Check out this item on Opeâ€¦\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "Standing alone at the end of the alley you see a  24 y/o Brass Dragonborn Weapon Master Fighter\n",
            "RT @DrJennyWilson: Please RT! We're hiring a technician to help with insect colonies and experiments with insect-transmitted plant virusesâ€¦\n",
            "RT @VST_Usilampatti: Intha Pongal Nammaku Super Collection mağŸ™ŒğŸ˜\n",
            "\n",
            "#Beast #BeastFirstSingle #Master  #MasterOnSunTv @actorvijay #VijaySocialTâ€¦\n",
            "RT @igtamil: Vaathi raidu ğŸ”¥ğŸ’¥\n",
            "\n",
            "#ThalapathyVijay #Thalapathy #Vijay #Master #VaathiRaidu #vaathicoming #TamilCinema https://t.co/hmKV1LLbye\n",
            "RT @nunudanaa: kihyun said when he first joined the company he found it very difficult for him to learn choreographies and he just couldn'tâ€¦\n",
            "RT @BeastFilmOffl: #Master Thalapathy Unseen Pic ğŸ’¥âœ¨ #Beast @actorvijay https://t.co/r0lOyCXrfc\n",
            "@holochronicles They look of cheaper build than the Master Replica ones but I'd have to see one to really know.\n",
            "@PvanHouwelingen @DeBlauwePen @SCPonderzoek @BNR @Lisawesterveld Oversterfte, Here now Netherlands:Sweden 2021. Noâ€¦ https://t.co/8ba7FHU21F\n",
            "@CryptoTownEU @TamilCryptoSchl \n",
            "@eusdnetwork\n",
            "RT @BhuppsBi: @VootSelect King master\n",
            "YOUTH ICON KARAN \n",
            "#KaranIsTheBoss #KaranKundrra \n",
            "@justvoot\n",
            " \n",
            "@VootSelect\n",
            " \n",
            "@OrmaxMedia\n",
            "RT @Danieljhonnyof1: Master or slave ğŸ˜ˆ ğŸ˜‡ https://t.co/fkcCcZ5zqH https://t.co/88ppaZtrjt\n",
            "RT @misterminshu: @staceyabrams We need 50 Stacey Abrams one per each state..\n",
            "We need an STACEY ABRAMS MASTER CLASS\n",
            "We need help in Floridaâ€¦\n",
            "RT @DHFV_Off: Thalapathy Vijay's #Master amazed â‚¹254.55 Crore worldwide gross and ranks 3rd biggest Indian Grosser globally in 2021 ğŸ˜ @Beasâ€¦\n",
            "RT @TeaPainUSA: All over America tonight, people that think theyâ€™re part of a master race are drinkinâ€™ their own pee.\n",
            "@THE_MAGNATE Perfect lesson from a Chart Master &amp; you are right lately we are seeing ,,BIG,, accounts charting 3-5Dâ€¦ https://t.co/sgt8yHGXy7\n",
            "RT @KKRiders: That moment when a classic move in Test cricket actually reminds you of a T20 master stroke! \n",
            "\n",
            "#Ashes #KKR #AmiKKR #AUSvENG hâ€¦\n",
            "RT @SomPundit: Ironically, these NGO hacks are mad at these gentlemen for overshadowing their white master Matt Bryden whom they have no quâ€¦\n",
            "I've just been informed that some segment of the right wing has come to the conclusion that Biden is behind the JFKâ€¦ https://t.co/IqRnzjdLnB\n",
            "RT @Ay_bkini: LOOL! \n",
            "\n",
            "I remember when we said Ambode wasnâ€™t good enough, they disagreed. \n",
            "\n",
            "Few months later, they started claiming Ambode iâ€¦\n",
            "RT @insideosunstate: #ForABetterOsun \n",
            "\n",
            "OSUN TOURISM: OYETOLA UNVEILS MASTER PLAN, HONOURS STAKEHOLDERS\n",
            "\n",
            "https://t.co/cGGlyWy9Zr /1 https://â€¦\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongsideâ€¦\n",
            "RT @miniminicult: haru, jiminâ€™s character, is an immortal guardian who used to be a rock &amp; has the qualities of a master ğŸ¤¯ https://t.co/qSeâ€¦\n",
            "RT @ChennaiTimesTOI: #Kaithi and #Master actor #ArjunDas elated about his parents meeting actor #Suriya, who they admire a lot!\n",
            "\n",
            "https://t.â€¦\n",
            "Well, that's another book full of Old Master drawings on its way...\n",
            "\n",
            "#BookAddict #Art #Books #Bibliophile\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "RT @insideosunstate: Osun State government has set the pace for its tourism development with Governor Adegboyega Oyetola disclosing that thâ€¦\n",
            "RT @aeiounis_: The power of du'a masyaAllah ğŸ¥ºğŸ¥°ğŸ’–âœ¨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, theâ€¦\n",
            "@yuzi_chahal https://t.co/pL9TGKPbFy\n",
            "ğŸ™ğŸ»âš¡ï¸âœ¨\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongsideâ€¦\n",
            "Dawg she was NICE with it too like I had to Google if she was actually blind lol\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "RT @JiminGlobal: 7FATES: CHAKHO Interview | Jimin\n",
            "\n",
            "Jimin tells us more about his character, Haru, and how he has the qualities of a master.â€¦\n",
            "RT @jboxbr: Games | â€˜Yu-Gi-Oh! Master Duelâ€™ terÃ¡ versÃ£o em portuguÃªs, segundo site oficial. O vindouro game serÃ¡ a versÃ£o oficial digital (â€¦\n",
            "Surely, we, the pioneers of science, can use it for good. We're the champions of discovery. Why fear it when we can master it?\n",
            "ithulam eppo nadakumo namma life la ğŸ¥²\n",
            "\n",
            "@actorvijay @Samanthaprabhu2 #beast #theri #master #Samantha #BeastFromApril https://t.co/7e0roLQLvl\n",
            "RT @PamplingNews: Haz RT y sÃ­guenos, sorteamos 15 camisetas cada mes.\n",
            "ğŸ†• Master Chief Â¡solo 10â‚¬ durante 24H!\n",
            "ConsÃ­guela ya ğŸ‘‰ https://t.co/3Râ€¦\n",
            "RT @aeiounis_: The power of du'a masyaAllah ğŸ¥ºğŸ¥°ğŸ’–âœ¨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, theâ€¦\n",
            "RT @ChennaiTimesTOI: #Kaithi and #Master actor #ArjunDas elated about his parents meeting actor #Suriya, who they admire a lot!\n",
            "\n",
            "https://t.â€¦\n",
            "Como que a pessoa te jura amor e depois volta pra ex? GENTE, falsidade nÃ­vel master.\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "ã‚¯ãƒ©ãƒ­ãƒ¯éšœãŠ™ï¸è€…ãƒ©ãƒ³ã‚­ãƒ³ã‚°â™¿ï¸\n",
            "1ä½G master ã—ãŠã‚“\n",
            "2ä½ã‚Šã‚“ã”åŠã‚„\n",
            "3ä½ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¦ãƒ«ãƒ•\n",
            "#MOONGLOWPHUPDATES\n",
            "\n",
            "MASS UPDATES AS OF 01.11.2022\n",
            "\n",
            "Master Tracker: https://t.co/rbQttbwcbo\n",
            "\n",
            "For questions, donâ€™t heâ€¦ https://t.co/WzPjIYMqKu\n",
            "I am the least adept with a bow. And that is precisely why I must master it.\n",
            "@CHARANJITCHANNI You have now become a master politician.  you are better actor than @ArvindKejriwal .  Great performance.\n",
            "RT @Iam_Jathu: Here's the Special Design  1 Years of  Block Buster  #Master  @Team_CDT ! â¤ \n",
            "@actorvijay #Beast  \n",
            "#1yearsofBlockBusterMasterâ€¦\n",
            "RT @KKRiders: That moment when a classic move in Test cricket actually reminds you of a T20 master stroke! \n",
            "\n",
            "#Ashes #KKR #AmiKKR #AUSvENG hâ€¦\n",
            "RT @Vijaysubbu1: If your boss 56\" can pose for camera doing meditation with his spectacles inside the cave, this is very much possible. Canâ€¦\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongsideâ€¦\n",
            "@Apple__iTV\n",
            "@zhitnikoleks @amalini2013\n",
            "RT @Velu_me: #BeastFirstSingle Trending 18.5K Tweets ğŸ˜³ğŸ”¥\n",
            "#Master  @actorvijay #Thalapathy66 #Beast\n",
            "@INCIndia https://t.co/pL9TGKPbFy\n",
            "ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã ã‘ã©ãƒ•ãƒ«ã‚³ãƒ³å‡ºæ¥ãŸã®â€¼ï¸\n",
            "MASTERã¯ã€ç„¡ç†ãã†ï¼ˆç¬‘ï¼‰ https://t.co/wpmXZbU0Fx\n",
            "Lan SiZhui tried to contain himself, but his voice sounded delighted, â€œYoung Master Mo, you are also here? Then isâ€¦ https://t.co/PxFGpqcwCR\n",
            "RT @aeiounis_: The power of du'a masyaAllah ğŸ¥ºğŸ¥°ğŸ’–âœ¨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, theâ€¦\n",
            "Starting Your Own Business? Hereâ€™s The Top Seven Skills You Need ToÂ Master https://t.co/DgPkmsTVxY\n",
            "Regarder le film original, une master class infos sÃ»res\n",
            "#CIO as business partner: 4 prerequisites IT leaders must master\n",
            "\n",
            "https://t.co/Fkxvh4SGGo\n",
            "\n",
            "Basically, ability to exâ€¦ https://t.co/UHNysbEWyj\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "Warm, real, or just a cold and lonely, lovely work of art? After 500 years, the Mona Lisa still incites criminal paâ€¦ https://t.co/w71JMsJzt5\n",
            "RT @PornKiwi: Part 11 of 69 ! \n",
            "\n",
            " Full video here (Title: Tom of Finland: Master Cut): https://t.co/iNehDwstb4 \n",
            "\n",
            " Follow @NewPornKiwi, @Pornâ€¦\n",
            "RT @AcademyOfFags: Please, Master... Please...\n",
            "\n",
            "[Follow backup page @academyoffags2] https://t.co/yOKZAEqDho\n",
            "Pas Naik Angkot Di halo mister wer ar yu going-in\n",
            "\n",
            "\"Bade Ka cijerah, A. Tiasa?\"\n",
            "\n",
            "Bapaknya orang Garut kuliah amerikâ€¦ https://t.co/X3XR8TsJ90\n",
            "Master Swim Club Tuesdays &amp; Thursdays 5:30-7 p.m.\n",
            "Starts today! Fees: AD, RET, RES, DEP (18+) free; DoD civilians -â€¦ https://t.co/bZ9wjokFfm\n",
            "RT @constads: - Î¤Î¹ ÏƒÎµ Ï†Î­ÏÎ½ÎµÎ¹ ÏƒÏ„Î·Î½ ÎµÎ»Î»Î·Î½Î¹ÎºÎ® Î´Î·Î¼Î¿ÏƒÎ¹Î¿Î³ÏÎ±Ï†Î¯Î±;\n",
            "RT @Team_CDT: 1 Year Of Blockbuster #Master ğŸ”¥ğŸ˜\n",
            "Design By @Iam_Jathu @Team_CDT \n",
            "#Beast @actorvijay https://t.co/CICqmo468R\n",
            "RT @RuazaINC: Prior 2 Corona, should some 1 write even a single sentence/tweet against Bluffs Master Modi, 1000s of Rs 2/tweet Bhakts willâ€¦\n",
            "ğŸ‘€\n",
            "RT @AkMaturity: Old @AjithMaturity Sila ğŸ¢gala Report Adichi Suspended Aachi This my new id oru support tweet pottu vidunga ğŸ’ª\n",
            "\n",
            "#Beast @actorâ€¦\n",
            "RT @RuazaINC: à¤‰à¤¸ Bluff Master à¤•à¥‡ \n",
            "à¤•à¤¹à¤¨à¥‡ à¤•à¥‹ à¤¤à¥‹ 70+ millions followers à¤¹à¥ˆà¤‚\n",
            "@Twitter \n",
            "à¤ªà¤° à¤•à¤­à¥€ à¤•à¤¿à¤¸à¥€ à¤¨à¥‡ \n",
            "à¤‰à¤¨ 70+ millions à¤•à¥‡ 1 % à¤¯à¤¾à¤¨à¤¿ \n",
            "700,000 (7 Lâ€¦\n",
            "#GodiModiDonoFail\n",
            "@GOPLeader Tell that to your master, Putin, you low life traitor!\n",
            "RT @aeiounis_: The power of du'a masyaAllah ğŸ¥ºğŸ¥°ğŸ’–âœ¨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, theâ€¦\n",
            "RT @Team_CDT: 1 Year Of Blockbuster #Master ğŸ”¥ğŸ˜\n",
            "Design By @Iam_Jathu @Team_CDT \n",
            "#Beast @actorvijay https://t.co/CICqmo468R\n",
            "RT @LuvDatta_INC: In Political Science we studied that a bunch of Ministers Sail together &amp; Sink together. Nowadays Godi Media too is incluâ€¦\n",
            "RT @dairingtia: elle a Ã©tÃ© virÃ©e parce quâ€™elle refusait les avances de son responsable donc PLEASE, si vous avez un contact pour elle :\n",
            "RT @stussyjimin: jimin: â€œ#HARU he was a rockâ€¦ then that rock turned human. he has the character that has the qualities of a masterâ€ https:/â€¦\n",
            "RT @Simple_Livinger: Today's #GodMorningTuesday\n",
            "\n",
            "SUPREME GOD\n",
            "\n",
            "In all our religious texts and scriptures, the glory of that one Prabhu/Masteâ€¦\n",
            "In case anyone else feels like skipping today, don't forget these  tips from the master.\n",
            "\n",
            "\"It's a little childish aâ€¦ https://t.co/1fcXXD0Uzw\n",
            "RT @RuazaINC: Well Said\n",
            "He turned out to be biggest Bluffs Master of the Nation and Liar.\n",
            "@swachhhyd https://t.co/pL9TGKPbFy\n",
            "RT @SEUNGM1NE: hyunjin : â€œu look like a young masterâ€\n",
            "also hyunjin : â€œa mindless young masterâ€\n",
            "\n",
            "seungmin was ready to fightğŸ˜†ğŸ˜†ğŸ˜†ğŸ˜† https://t.câ€¦\n",
            "RT @Williamz902: I can't help feel that this is a sinful waste of money.\n",
            "\n",
            "https://t.co/SlM2KH84KM\n",
            "RT @PamplingNews: Haz RT y sÃ­guenos, sorteamos 15 camisetas cada mes.\n",
            "ğŸ†• Master Chief Â¡solo 10â‚¬ durante 24H!\n",
            "ConsÃ­guela ya ğŸ‘‰ https://t.co/3Râ€¦\n",
            "Jveux monter ce compte master si quelqu'un a un compte Ã  monter aussi et veut duo go dm \n",
            "\n",
            "https://t.co/Y0LRLAYJeS\n",
            "RT @frhnaism: Pertengahan tahun lepas i prayed â€œya Allah, permudahkan urusan jodoh aku. kalau sampai umur 28 aku tak kahwin, aku memang takâ€¦\n",
            "RT @TamilVi17140583: ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£\n",
            "RT @FranQAnde: Fortunately and unfortunately to make Nigeria work we mostly have to be like Tunde the chess master, one by one we must fixâ€¦\n",
            "RT @13phiIe: haru has the qualities of a master !  https://t.co/uKci80H3Uw\n",
            "RT @nunudanaa: kihyun said when he first joined the company he found it very difficult for him to learn choreographies and he just couldn'tâ€¦\n",
            "It was inevitable.  Their Fuhrer drinks pee, Russian hooker pee.  Best in the world!  Produce of champagne and caviâ€¦ https://t.co/QKgzAwzKv1\n",
            "How do you focus and master your vision in life?\n",
            "\n",
            "(Source:  E/a)\n",
            "\n",
            "For ideas, try:\n",
            "\n",
            "- Law of Success book - Hill\n",
            "- Tâ€¦ https://t.co/2Y2HXyLXrq\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "RT @banzai_TRPG: ğŸ’« ì„¸ë² ë£¨ìŠ¤ ìŠ¤ë„¤ì´í”„, í˜¸ê·¸ì™€íŠ¸ì— ëŒì•„ì˜¨ ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.\n",
            "RT @nunudanaa: kihyun said when he first joined the company he found it very difficult for him to learn choreographies and he just couldn'tâ€¦\n",
            "RT @WasimJaffer14: Captain, Opener, No.3, Wicket Keeper, ICC event winning coach, former NCA head, and now India coach. But above all a greâ€¦\n",
            "RT @moonglowphCS: #MOONGLOWPHUPDATES\n",
            "\n",
            "MASS UPDATES AS OF 01.11.2022\n",
            "\n",
            "Master Tracker: https://t.co/rbQttbwcbo\n",
            "\n",
            "For questions, donâ€™t hesitateâ€¦\n",
            "@iams_eth master GIF\n",
            "RT @KAILASA_UN: \"Order is chaos.\n",
            "There is a beautiful Zen story: \n",
            "A disciple goes to a Zen master and asks, \n",
            "â€ŸMaster, please explain to meâ€¦\n",
            "\"We cannot always control our thoughts, but we can control our words, and repetition impresses the subconscious, anâ€¦ https://t.co/NMVL8egZoZ\n",
            "RT @ammisays: i donâ€™t know whether the idea that jimin is a master of witchcraft or that heâ€™s a powerful shapeshifter makes me more excitedâ€¦\n",
            "RT @UDCEHD: What is it like to be an education or human development graduate student at UD? Students from our Master's and Ed.D. programs wâ€¦\n",
            "ã€ ç‹¬ã‚Šã‚“ã¼ã‚¨ãƒ³ãƒ´ã‚£ãƒ¼ ã€\n",
            "\n",
            "é›£æ˜“åº¦ï¼šMASTER\n",
            "å˜˜wã™ã”ã„é€Ÿã•ã§çµ‚ã‚ã‚‹ã‚“ã ã‘ã©wæ˜æ—¥ã‚„ã‚‹ã‚\n",
            "#ãƒ—ãƒ­ã‚»ã‚« æ›²ãƒœã‚¿ãƒ³\n",
            "#ã¿ã‚“ãªã®ãƒœã‚¿ãƒ³ãƒ¡ãƒ¼ã‚«ãƒ¼\n",
            " https://t.co/dPlTqBdaCf\n",
            "RT @Sissy_Trainers: If Your Master Give You Two Option For Sissygasm\n",
            "1-CuM Through AnaL By DilDo Or\n",
            "2-CuM Through AnaL By His CocK \n",
            "&amp; Ask Uâ€¦\n",
            "@ZakaWaqar @ImranKhanPTI #ImranKhanPTI respected sir please hat jor k Ap say request ha main ak mother hu mari jobâ€¦ https://t.co/98FjFEidod\n",
            "RT @GersonKoringa: \"ManchÃ©ster unaiti\"\n",
            "\"Master Chef United\n",
            "\"Manchester Unlait\"\n",
            "\n",
            "Simplesmente El gordo Braz poliglota kkkkkkkkk https://t.coâ€¦\n",
            "RT @HayateManazili: ã€MADã€‘åˆä»£ARMORED COREï¼†ARMORED CORE MASTER OF ARENA https://t.co/wfsfJRBaql #sm249748 #ãƒ‹ã‚³ãƒ‹ã‚³å‹•ç”»\n",
            "ã“ã®ç±³å¥½ã https://t.co/oiJiGRNp5x\n",
            "RT @imas_columbia: ã€Œæ˜Ÿç’°ä¸–ç•Œã€æ˜æ—¥ç™ºå£²ğŸš€ğŸ’«\n",
            "\n",
            "æ–°ã‚·ãƒªãƒ¼ã‚ºã€ŒTHE IDOLM@STER CINDERELLA GIRLS STARLIGHT MASTER R/LOCK ON! 01 æ˜Ÿç’°ä¸–ç•Œã€\n",
            "\n",
            "Starring by\n",
            "#ç ‚å¡šã‚ãã‚‰ #é€Ÿæ°´å¥ #é«˜å£æ¥“ #â€¦\n",
            "RT @btsqtsarchive: #BTS In 7Fates: CHAKHO \n",
            "\n",
            "#ZEHA_JungKook is half tiger, half human and has a tiger power fighting against beoms alongsideâ€¦\n",
            "RT @Sissy_Trainers: Retweet If You Love To Watch Or Want To Watch Your Clitty Flip Flop In The Mirror When You Ride On A CocK Or On A DilDoâ€¦\n",
            "RT @aeiounis_: The power of du'a masyaAllah ğŸ¥ºğŸ¥°ğŸ’–âœ¨ \n",
            "\n",
            "Du'a is conversation with Allah, our Creator, our Lord and Master, the All Knowing, theâ€¦\n",
            "RT @Mkumari99: #GodmorningMonday\n",
            "Sant Rampal Ji Maharaj is the only spiritual master in the world who has the right to conduct satsang andâ€¦\n",
            "Twitter is a great place for secrets, right? I'm sure I can trust you with this one... ğŸ¤«\n",
            "\n",
            "https://t.co/O4esQdpwFq\n",
            "RT @wimipolis: Ich: 39, Ã¶rtlich flexibel, Master MilitÃ¤rgeschichte/MilitÃ¤rsoziologie, fast 20 Jahre Arbeitserfahrung in Wissenschaft, Politâ€¦\n",
            "RT @NewPornKiwi: Part 17 of 69 ! \n",
            "\n",
            " Full video here (Title: Tom of Finland: Master Cut): https://t.co/IrauEVCV9H \n",
            "\n",
            " Follow @NewPornKiwi, @Pâ€¦\n",
            "@1970euro @robfoot And not even a librarian. He teaches at the school of library science at U of T. Teaches. A Mastâ€¦ https://t.co/oTjfUesy3P\n",
            "I want to master two lyrical things in working on and strengthen my vocal cords so I can have breathing control whiâ€¦ https://t.co/x7xgPI675S\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-9bde13b980d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtwitterStream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtwitterStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"master\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filter_level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delimited'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, is_async)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[0;34m(self, resp)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mstripped_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mline\u001b[0m \u001b[0;31m# line is sometimes None so we need to check here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstripped_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[0;34m(self, sep)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Read the next chunk size from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBBombq6GgV"
      },
      "source": [
        "# from tweepy import Stream\n",
        "# from tweepy import OAuthHandler\n",
        "# from tweepy.streaming import StreamListener\n",
        "# import json\n",
        "# import sent_mod1 as s\n",
        "\n",
        "# #consumer key, consumer secret, access token, access secret.\n",
        "# ckey=\"XXeG5cy7QUfahiOHlacBaddIF\"\n",
        "# csecret=\"quSrqvVZUfRqbp8SZ0bDZimz8avlS96htsonlG1dFAehgsmMI1\"\n",
        "# atoken=\"1399846090699067392-W14id1VCFiLIib3MD7kaT2C6pvDmvX\"\n",
        "# asecret=\"6nrOLFSELerY9HzGSV5YGNigZnhSM54izskj2iahnFina\"\n",
        "\n",
        "# #from twitterapistuff import *\n",
        "\n",
        "# class listener(StreamListener):\n",
        "#   def on_data(self, data):\n",
        "#     all_data = json.loads(data)\n",
        "#     tweet = all_data[\"text\"]\n",
        "#     sentiment_value, confidence = s.sentiment(tweet)\n",
        "#     print(tweet, sentiment_value)\n",
        "#     if sentiment_value==\"pos\":\n",
        "#       output = open(\"twitter_pos_out.txt\",\"a\")\n",
        "#       output.write(tweet)\n",
        "#       output.write('\\n')\n",
        "#       output.close()\n",
        "#     else:\n",
        "#       output = open(\"twitter_neg_out.txt\",\"a\")\n",
        "#       output.write(tweet)\n",
        "#       output.write('\\n')\n",
        "#       output.close()\n",
        "#     return True\n",
        "\n",
        "#   def on_error(self, status):\n",
        "#     print(status)\n",
        "\n",
        "# auth = OAuthHandler(ckey, csecret)\n",
        "# auth.set_access_token(atoken, asecret)\n",
        "\n",
        "# twitterStream = Stream(auth, listener())\n",
        "# twitterStream.filter(track=[\"rajini\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyf5XdENdnYo"
      },
      "source": [
        "Code switching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkNlDTglhMzS",
        "outputId": "b6205fcf-3385-417b-ab25-a44ed1c6cef5"
      },
      "source": [
        "pip install langdetect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
            "\r\u001b[K     |â–                               | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |â–Š                               | 20kB 22.1MB/s eta 0:00:01\r\u001b[K     |â–ˆ                               | 30kB 25.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–                              | 40kB 18.8MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–Š                              | 51kB 16.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆ                              | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–                             | 71kB 13.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–Š                             | 81kB 14.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆ                             | 92kB 14.8MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–                            | 102kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–Š                            | 112kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆ                            | 122kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–                           | 133kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 143kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 153kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 163kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 174kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 184kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 194kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 204kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 215kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 225kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 235kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 245kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 256kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 266kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 276kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 286kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 296kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 307kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 317kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 327kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 337kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 348kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 358kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 368kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 378kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 389kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 399kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 409kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 419kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 430kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 440kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 450kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 460kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 471kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 481kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 491kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 501kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 512kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 522kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 532kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š              | 542kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 552kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 563kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š             | 573kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 583kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 593kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 604kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 614kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 624kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 634kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 645kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 655kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 665kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 675kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 686kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 696kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 706kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 716kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 727kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 737kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 747kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 757kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 768kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 778kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 788kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 798kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 808kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 819kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 829kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 839kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 849kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 860kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 870kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 880kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 890kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 901kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 911kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 921kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 931kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 942kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 952kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 962kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 972kB 10.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 983kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993242 sha256=3c4dda5642c801ee2d5aa6f5f005ed9c3da8bd0ee528022aefad2e63fd192581\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xbOiswdhT6H",
        "outputId": "3e05d32f-a945-47d2-8d77-44d7733d4841"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 5.9MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/b5/96947391e84e332010694ac8c4ba64841b90301ee0d05e7236ff934bb9f6/hstspreload-2021.7.5-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 22.1MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.5.30)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 7.1MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 7.5MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=b539242428d4bf29b20e00e45f6f6cc949100996192f58a8ddf065083a164be7\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hstspreload, rfc3986, sniffio, h11, hpack, hyperframe, h2, httpcore, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.7.5 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBTm0u9K9U6k"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLG1jhNM9ZCF"
      },
      "source": [
        "LinearSVC_classifier = SklearnClassifier(LinearSVC())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u1Um1gp9ziM"
      },
      "source": [
        "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dho5_nQm-MUO"
      },
      "source": [
        "SGDC_classifier = SklearnClassifier(SGDClassifier())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sME2qeJzhbln"
      },
      "source": [
        "dataset = pd.read_excel('/content/drive/MyDrive/Research/NLTK VIthu (2).xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWNGB4Alh6G8"
      },
      "source": [
        "#Cleaning the text\n",
        "def tokenizer(sentence):\n",
        "    mytokens=word_tokenize(sentence)\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbcUiSAvh9at"
      },
      "source": [
        "#Cleaning text\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        return [clean_text(text) for text in X]\n",
        "    def fit(self, X, y, **fit_params):\n",
        "        return self\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text \n",
        "def clean_text(text):     \n",
        "    return text.strip().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmiPRvQpiBE6"
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,1)) \n",
        "tfvectorizer = TfidfVectorizer(tokenizer = tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsvtGarEiERU"
      },
      "source": [
        "X = dataset['Text']\n",
        "y = dataset['Language']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=77)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LbvpWKiG_Y",
        "outputId": "a28c9f47-e454-4eee-a88a-e97f172e4c24"
      },
      "source": [
        "#Using third model\n",
        "SVCclassifier = LinearSVC()\n",
        "SVCmodel = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', SVCclassifier)])\n",
        "\n",
        "# Train the Model\n",
        "SVCmodel.fit(X_train,y_train)   \n",
        "pred3= SVCmodel.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[296   1   0]\n",
            " [  0 309   0]\n",
            " [  4  11 279]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       0.99      1.00      0.99       297\n",
            "       Tamil       0.96      1.00      0.98       309\n",
            "   Thanglish       1.00      0.95      0.97       294\n",
            "\n",
            "    accuracy                           0.98       900\n",
            "   macro avg       0.98      0.98      0.98       900\n",
            "weighted avg       0.98      0.98      0.98       900\n",
            "\n",
            "Accuracy: 98.22222222222223%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm6OfsdbASRz",
        "outputId": "ed30acff-3b2c-4264-d54b-6ee1e805440d"
      },
      "source": [
        "BernoulliNB = BernoulliNB()\n",
        "BernoulliNB_model = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', BernoulliNB)])\n",
        "\n",
        "# Train the Model\n",
        "BernoulliNB_model.fit(X_train,y_train)   \n",
        "pred3= BernoulliNB_model.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[265   3  29]\n",
            " [  0 309   0]\n",
            " [  0   2 292]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       1.00      0.89      0.94       297\n",
            "       Tamil       0.98      1.00      0.99       309\n",
            "   Thanglish       0.91      0.99      0.95       294\n",
            "\n",
            "    accuracy                           0.96       900\n",
            "   macro avg       0.96      0.96      0.96       900\n",
            "weighted avg       0.97      0.96      0.96       900\n",
            "\n",
            "Accuracy: 96.22222222222221%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmecjcFA7TVu",
        "outputId": "d3e54da5-12e2-4439-abf2-0f1433b7a96c"
      },
      "source": [
        "logesticRegression = LogisticRegression()\n",
        "logisticModel = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', logesticRegression)])\n",
        "# Train the Model\n",
        "logisticModel.fit(X_train,y_train)   \n",
        "pred3= logisticModel.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[291   4   2]\n",
            " [  0 308   1]\n",
            " [  4  13 277]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       0.99      0.98      0.98       297\n",
            "       Tamil       0.95      1.00      0.97       309\n",
            "   Thanglish       0.99      0.94      0.97       294\n",
            "\n",
            "    accuracy                           0.97       900\n",
            "   macro avg       0.97      0.97      0.97       900\n",
            "weighted avg       0.97      0.97      0.97       900\n",
            "\n",
            "Accuracy: 97.33333333333334%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOZ7vCR1-P1u",
        "outputId": "ba99fed3-4ad3-41ea-8340-cde8d0fce6d7"
      },
      "source": [
        "SGDClassifier = SGDClassifier()\n",
        "SGDV_Model = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', vectorizer),\n",
        "                 ('classifier', SGDClassifier)])\n",
        "# Train the Model\n",
        "SGDV_Model.fit(X_train,y_train)   \n",
        "pred3= SGDV_Model.predict(X_test)\n",
        "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,pred3)}')\n",
        "print(f'\\nClassification Report:\\n{classification_report(y_test,pred3)}')\n",
        "print(f'Accuracy: {accuracy_score(y_test,pred3)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[294   2   1]\n",
            " [  0 308   1]\n",
            " [  3  11 280]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     English       0.99      0.99      0.99       297\n",
            "       Tamil       0.96      1.00      0.98       309\n",
            "   Thanglish       0.99      0.95      0.97       294\n",
            "\n",
            "    accuracy                           0.98       900\n",
            "   macro avg       0.98      0.98      0.98       900\n",
            "weighted avg       0.98      0.98      0.98       900\n",
            "\n",
            "Accuracy: 98.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vmku1hbUi6L",
        "outputId": "c6dccdba-4ae8-4d75-a750-21d9ac81de6d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD7zkIu1iKxe",
        "outputId": "62ef0f97-57bb-49b2-ae37-7d63b0c2b162"
      },
      "source": [
        "pre = SVCmodel.predict([\"à®¯à®¾à®°à®¾à®µà®¤à¯ à®®à¯à®´à¯ à®šà¯Šà®±à¯à®±à¯Šà®Ÿà®°à¯ˆà®¯à¯à®®à¯ à®®à¯€à®£à¯à®Ÿà¯à®®à¯ à®®à¯€à®£à¯à®Ÿà¯à®®à¯ à®šà¯†à®¯à¯à®¯ à®µà®¿à®°à¯à®®à¯à®ªà®¿à®©à®¾à®²à¯ à®•à®Ÿà¯ˆà®šà®¿ à®µà®¾à®°à¯à®¤à¯à®¤à¯ˆà®¯à¯ˆ à®¨à®¾à®©à¯ à®ªà®¿à®Ÿà®¿à®•à¯à®•à®µà®¿à®²à¯à®²à¯ˆ à®à®©à¯à®ªà®¤à®¾à®²à¯ à®¨à¯€à®™à¯à®•à®³à¯ à®…à®¤à¯ˆ à®®à¯€à®£à¯à®Ÿà¯à®®à¯ à®šà¯†à®¯à¯à®¯ à®®à¯à®Ÿà®¿à®¯à¯à®®à®¾?\"])\n",
        "print(pre[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamil\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF-K9gwH_6wg",
        "outputId": "e41fb3c5-bc83-448f-fbcd-e33067f35ec2"
      },
      "source": [
        "pre = SGDV_Model.predict([\"à®¯à®¾à®°à®¾à®µà®¤à¯ à®®à¯à®´à¯ à®šà¯Šà®±à¯à®±à¯Šà®Ÿà®°à¯ˆà®¯à¯à®®à¯ à®®à¯€à®£à¯à®Ÿà¯à®®à¯ à®®à¯€à®£à¯à®Ÿà¯à®®à¯ à®šà¯†à®¯à¯à®¯ à®µà®¿à®°à¯à®®à¯à®ªà®¿à®©à®¾à®²à¯ à®•à®Ÿà¯ˆà®šà®¿ à®µà®¾à®°à¯à®¤à¯à®¤à¯ˆà®¯à¯ˆ à®¨à®¾à®©à¯ à®ªà®¿à®Ÿà®¿à®•à¯à®•à®µà®¿à®²à¯à®²à¯ˆ à®à®©à¯à®ªà®¤à®¾à®²à¯ à®¨à¯€à®™à¯à®•à®³à¯ à®…à®¤à¯ˆ à®®à¯€à®£à¯à®Ÿà¯à®®à¯ à®šà¯†à®¯à¯à®¯ à®®à¯à®Ÿà®¿à®¯à¯à®®à®¾?\"])\n",
        "print(pre[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xlOzNkHiP6S"
      },
      "source": [
        "t1=\"à®ªà®¤à¯à®¤ à®µà®šà¯à®š à®¨à¯†à®°à¯à®ªà¯à®ªà¯ à®ªà®°à®ªà®°à®©à¯à®©à¯ à®ªà®±à¯à®±à®¿ à®à®°à®¿à®µà®¤à¯ à®ªà¯‹à®² à®ªà®Ÿà®®à¯ à®“à®Ÿà¯à®•à®¿à®±à®¤à¯.. à®¤à¯Šà®Ÿà®•à¯à®•à®¤à¯à®¤à®¿à®²à®¿à®°à¯à®¨à¯à®¤à¯ à®‡à®±à¯à®¤à®¿ à®µà®°à¯ˆ.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Ig2vXjiSdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4756ec-da6c-4d73-f6d6-745b0cd472ea"
      },
      "source": [
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "print(detect(t1))\n",
        "translator= Translator()\n",
        "translatedSent= translator.translate(t1, src='ta', dest='en',)\n",
        "print(translatedSent.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ta\n",
            "The film runs like wildfire about the Patha Vachcha fire sensation .. from beginning to end.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uovC4Xfoe8Wi"
      },
      "source": [
        "Language detection and translation normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIAqqMCKiWUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c52f937-f548-4146-93f8-3c38bab98471"
      },
      "source": [
        "#from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "tweet_try=[\"vanakam thala padam vera level la irunthathu msg \"]\n",
        "\n",
        "\n",
        "pre = SVCmodel.predict(tweet_try)\n",
        "print(pre[0])\n",
        "if pre[0]==\"English\":\n",
        "  print(tweet_try[0])\n",
        "elif pre[0]==\"Thanglish\":\n",
        "  translator1= Translator()\n",
        "  translatedSent= translator1.translate(tweet_try[0], src='ta', dest='en')\n",
        "  print(translatedSent.text)  \n",
        "else:\n",
        "  translator= Translator()\n",
        "  translatedSent= translator.translate(tweet_try[0],src='ta',dest='en')\n",
        "  print(translatedSent.text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thanglish\n",
            "Hello head movie was on another level\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NztT7eere5FI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j35mMDVlg492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "4ae2921e-c087-4f8e-be57-05b3d469bf92"
      },
      "source": [
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "import sent_mod1 as s\n",
        "\n",
        "#consumer key, consumer secret, access token, access secret.\n",
        "ckey=\"XXeG5cy7QUfahiOHlacBaddIF\"\n",
        "csecret=\"quSrqvVZUfRqbp8SZ0bDZimz8avlS96htsonlG1dFAehgsmMI1\"\n",
        "atoken=\"1399846090699067392-W14id1VCFiLIib3MD7kaT2C6pvDmvX\"\n",
        "asecret=\"6nrOLFSELerY9HzGSV5YGNigZnhSM54izskj2iahnFina\"\n",
        "\n",
        "#from twitterapistuff import *\n",
        "\n",
        "class listener(StreamListener):\n",
        "  def on_data(self, data):\n",
        "    all_data = json.loads(data)\n",
        "    tweet = all_data[\"text\"]\n",
        "    tweet_try=[tweet]\n",
        "    pre = SVCmodel.predict(tweet_try)\n",
        "    if pre[0]==\"English\":\n",
        "      sentiment_value, confidence = s.sentiment(tweet)\n",
        "    elif pre[0]==\"Thanglish\":\n",
        "      translator1= Translator()\n",
        "      translatedSent= translator1.translate(tweet_try[0], src='ta', dest='en')\n",
        "      sentiment_value, confidence = s.sentiment('translatedSent')  \n",
        "    else:\n",
        "      translator= Translator()\n",
        "      translatedSent= translator.translate(tweet_try[0], src='ta', dest='en')\n",
        "      sentiment_value, confidence = s.sentiment('translatedSent')\n",
        "    # sentiment_value, confidence = s.sentiment(tweet)\n",
        "    print(tweet, sentiment_value)\n",
        "    if sentiment_value==\"pos\":\n",
        "      output = open(\"twitter_pos_out.txt\",\"a\")\n",
        "      output.write(tweet)\n",
        "      output.write('\\n')\n",
        "      output.close()\n",
        "    else:\n",
        "      output = open(\"twitter_neg_out.txt\",\"a\")\n",
        "      output.write(tweet)\n",
        "      output.write('\\n')\n",
        "      output.close()\n",
        "    return True\n",
        "\n",
        "  def on_error(self, status):\n",
        "    print(status)\n",
        "\n",
        "auth = OAuthHandler(ckey, csecret)\n",
        "auth.set_access_token(atoken, asecret)\n",
        "\n",
        "twitterStream = Stream(auth, listener())\n",
        "twitterStream.filter(track=[\"valimai\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5be117583b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamListener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msent_mod1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#consumer key, consumer secret, access token, access secret.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sent_mod1'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}